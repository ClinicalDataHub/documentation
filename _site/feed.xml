<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xml" href="http://healthcare.ai/feed.xslt.xml"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.3.1">Jekyll</generator><link href="http://healthcare.ai/feed.xml" rel="self" type="application/atom+xml" /><link href="http://healthcare.ai/" rel="alternate" type="text/html" /><updated>2017-01-19T17:38:20-07:00</updated><id>http://healthcare.ai//</id><title type="html">healthcare.ai</title><entry><title type="html">Survey of deep learning in radiology</title><link href="http://healthcare.ai/blog/2017/01/19/survey-of-deep-learning-in-radiology/" rel="alternate" type="text/html" title="Survey of deep learning in radiology" /><published>2017-01-19T16:32:11-07:00</published><updated>2017-01-19T16:32:11-07:00</updated><id>http://healthcare.ai/blog/2017/01/19/survey-of-deep-learning-in-radiology</id><content type="html" xml:base="http://healthcare.ai/blog/2017/01/19/survey-of-deep-learning-in-radiology/">&lt;p&gt;This blog has been talking a lot about Machine Learning (ML) with regard to tabular data. That makes sense because predictive algorithms based on tabular data are often easy to implement and have a lot of potential to improve outcomes. Also, we have access to a lot of tabular data from the EHR.  However, ML is capable of doing a lot more than predicting probabilities on tabular data, and there are incredible opportunities in other areas of healthcare. One in particular is in Radiology and Pathology departments. These departments generate tabular data, but their bread and butter is image data. Beth Israel Deaconess Medical Center (Harvard), for instance, generates approximately 20 terabytes of image data per year (vs. 1 TB text data) &lt;a href=&quot;http://geekdoctor.blogspot.com/2011/04/cost-of-storing-patient-records.html&quot;&gt;source&lt;/a&gt;. That’s a lot of data to potentially use!&lt;/p&gt;

&lt;p&gt;With all the talk around deep learning lately, it would be hard for an ML group to avoid at least discussing whether or not it would be practical to develop our own applications. While deep learning is absolutely a buzz word, there is a lot of potential behind the technique. In only a handful of years, deep learning will &lt;a href=&quot;https://www.tesla.com/autopilot&quot;&gt;driving our cars&lt;/a&gt;, doing &lt;a href=&quot;http://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html?smid=pl-share&amp;amp;_r=0&quot;&gt;real time translation of spoken language&lt;/a&gt;, allowing you to &lt;a href=&quot;https://www.ditto.com/&quot;&gt;virtually try on glasses online&lt;/a&gt;, and many other amazing things that we haven’t even imagined yet. With this kind of limitless potential, it isn’t hard to imagine an opportunity for computers to read medical images, as outlined in this paper from the &lt;a href=&quot;http://www.nejm.org/doi/full/10.1056/NEJMp1606181&quot;&gt;New England Journal of Medicine&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;One of the great things about working on &lt;a href=&quot;http://www.healthcare.ai&quot;&gt;healthcare.ai&lt;/a&gt; is the opportunity to plan and shape &lt;a href=&quot;http://healthcare.ai/blog/2016/12/21/which-algorithms-are-in-healthcareai/&quot;&gt;what the package is capable of&lt;/a&gt;. There are things that we are working towards in the near future, like unsupervised learning, and there is functionality that is still not mainstream and will require a more concentrated research effort for us to be able to implement. &lt;strong&gt;The purpose of this post is to try to provide a survey of machine learning on medical imaging, and where the lowest hanging fruit might be.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;One area, &lt;a href=&quot;https://www.cancer.gov/types/breast/mammograms-fact-sheet#q1&quot;&gt;mammographic screening&lt;/a&gt;, is a particularly promising application of deep learning. Women are recommended to get an annual mammogram for the later 30 or 40 years of their lives, the images are consistent, and they take a lot of radiologist time to evaluate. Take a look at &lt;a href=&quot;http://ashevillegynecologywellness.com/wp-content/uploads/2016/05/MammoGram-Seriesshow.jpg&quot;&gt;these images&lt;/a&gt;. Even if you are not a trained radiologist, you are probably able to make medically relevant observations about them. The normal mammogram looks rather indistinct. There are brighter areas and darker areas, but they are mostly linear. The second image from the left has a large, round, and uniformly bright spot, almost a sure sign of a cyst. Finally, the cancer image has a very bright spot that seems to have tentacles extending from a central structure. While real images can be extremely subtle and difficult to read (that’s why you need an MD!), a radiologist would perform the same steps you just did. They make observations about the image, relate their findings to their medical knowledge, and make a diagnosis.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/radPost_mammo.png&quot; alt=&quot;Example Mammograms&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As it turns out, &lt;a href=&quot;http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html&quot;&gt;deep learning algorithms&lt;/a&gt; excel at the same kind of analysis and pattern recognition that a radiologist does. The algorithms will examine thousands of images of every type and eventually learn how to distinguish them based on different sized sections of the image. There is potential to see patterns that would elude even best radiologists, detect exquisitely specific changes in a single person’s history, and totally transform radiology. This application could help to treat breast cancer more effectively, at lower cost, with less resources.
Of course, there have already been efforts at individual academic medical centers in mammographic screening. Big breakthroughs in breast imaging are just starting to come out in the high-impact journals.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Recently, &lt;a href=&quot;http://www.nature.com/articles/srep27327&quot;&gt;Wang et al.&lt;/a&gt; were able to detect &lt;a href=&quot;http://www.mayoclinic.org/symptoms/breast-calcifications/basics/definition/sym-20050834&quot;&gt;microcalcifications&lt;/a&gt; in mammograms, in effective early indicator of breast cancer. They used a data set of approximately 1200 images to train a Stacked Auto-Encoder deep learning architecture, and were able to identify microcalcifications with an &lt;a href=&quot;http://healthcare.ai/blog/2016/12/15/model-evaluation-using-roc-curves/&quot;&gt;AUC&lt;/a&gt; of 0.87.&lt;/li&gt;
  &lt;li&gt;In another study, &lt;a href=&quot;http://www.nature.com/articles/srep24454&quot;&gt;Cheng et al.&lt;/a&gt; used a Stacked Denoising Auto-Encoder to differentiate malignant breast lesions from benign in 550 &lt;a href=&quot;http://www.cancer.org/cancer/breast-cancer/screening-tests-and-early-detection/breast-ultrasound.html&quot;&gt;ultrasound&lt;/a&gt; images. They achieved an AUC of 0.90 for the task.&lt;/li&gt;
  &lt;li&gt;Finally, &lt;a href=&quot;https://arxiv.org/abs/1612.00542&quot;&gt;Levi et al.&lt;/a&gt; trained several existing Convolutional Neural Network (CNN) architectures to classify pre-labeled regions from 1800 mammograms. Their GoogLeNet CNN was able to distinguish malignant from benign with precision of 0.92 and recall of 0.93.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If we were to move forward with a deep learning model for breast imaging, reproducing one of these studies would be a good starting point. Screening mammograms are probably the best place to start, due to their high volume in healthcare. In addition to breast imaging, &lt;a href=&quot;http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7463094&quot;&gt;this excellent review&lt;/a&gt; points out several other promising areas to begin. Additional promising areas are in lung cancer screening from CT images, and in automated tissue labeling of brain MRI. In lung cancer screening and automatic brain segmentation,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/ftp/arxiv/papers/1611/1611.06651.pdf&quot;&gt;Yang et al.&lt;/a&gt; employed a CNN to classify known lung nodules as either malignant or benign from 4 separate data sets. Some of their models failed to generalize well, but others could achieve extremely high classification accuracy on the validation set.&lt;/li&gt;
  &lt;li&gt;In the same paper as their breast study, &lt;a href=&quot;http://www.nature.com/articles/srep24454&quot;&gt;Cheng et al.&lt;/a&gt; classified 1400 lung nodules from chest CT images. They saw AUC as high as 0.98 depending on the algorithm configuration.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S1361841516300330&quot;&gt;Havaei et al.&lt;/a&gt; present their state of the art in segmentation of glioblastoma brain tumors from MRI images. They use a CNN to segment a competition data set with practical speed (0.5 to 3 minutes) and high accuracy.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It would be crazy to try to cover every bit of research in this post. There are countless studies to cover beyond those discussed here, plus annual conferences such as &lt;a href=&quot;https://nips.cc/&quot;&gt;NIPS&lt;/a&gt; and &lt;a href=&quot;https://2017.icml.cc/&quot;&gt;ICML&lt;/a&gt;. However, it would be worth mentioning a few areas of research from the private sector.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Google Deepmind &lt;a href=&quot;https://deepmind.com/applied/deepmind-health/&quot;&gt;recently announced a health initiative&lt;/a&gt; and has been working to identify diabetic retinopathy from images of the eye.&lt;/li&gt;
  &lt;li&gt;The San Francisco based start-up, &lt;a href=&quot;http://www.enlitic.com/&quot;&gt;Enlitic&lt;/a&gt;, works to develop deep learning methods for the use cases above and has partnered with several hospitals already.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://siris-medical.com/&quot;&gt;Siris Medical&lt;/a&gt; works to use past patient data to automate radiation treatment planning and brain segmentation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, many of the data sets that were used in the studies mentioned above are open source and freely available.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.visceral.eu/&quot;&gt;Visceral&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://image-net.org/&quot;&gt;ImageNet&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/c/data-science-bowl-2017&quot;&gt;Kaggle Data Science Bowl&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://grand-challenge.org/All_Challenges/&quot;&gt;Grand Challenge&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://imaging.cancer.gov/programsandresources/informationsystems/lidc&quot;&gt;Lung Image Database Consortium&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://braintumorsegmentation.org/&quot;&gt;Multimodal Brain Tumor Segmentation Challenge&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Breast screening, lung screening, and brain segmentation are the some of the most popular and promising applications of deep learning in healthcare currently. Early research in these areas has shown encouraging results, and there is a lot of effort towards advancements. Furthermore, these areas have the potential to benefit a lot of patients as they include common diseases, treatments, and procedures. It’s an exciting time to be in healthcare machine learning, and we look forward to implementing deep learning into our package.   Thanks for reading, and please feel free to &lt;a href=&quot;http://healthcare.ai/contact&quot;&gt;reach out&lt;/a&gt; with questions!&lt;/p&gt;</content><author><name>Mike Mastanduno</name></author><category term="literature" /><summary type="html">This blog will surveys recent research in deep learning and radiology.</summary></entry><entry><title type="html">Using R for healthcare data analysis</title><link href="http://healthcare.ai/blog/2017/01/17/using-r-for-data-analysis/" rel="alternate" type="text/html" title="Using R for healthcare data analysis" /><published>2017-01-17T11:12:11-07:00</published><updated>2017-01-17T11:12:11-07:00</updated><id>http://healthcare.ai/blog/2017/01/17/using-r-for-data-analysis</id><content type="html" xml:base="http://healthcare.ai/blog/2017/01/17/using-r-for-data-analysis/">&lt;p&gt;When working with data in healthcare, business intelligence (BI) folks often turn to tools like Excel, SSMS, Tableau, and Qlik. Typically, multiple tools will be used when analyzing a dataset. Sometimes the analyst will use Excel to look at the data, get a sense for how the column are distributed, perhaps make a histogram or scatterplot. Often, analysts will later turn to Qlik and/or Tableau to provide an interactive app, often hosted on a dedicated server, such that folks in other departments can explore the same data. In this same process, the analyst or data architect may start by querying the database in SSMS as well, to do some simple counts and group by’s, in an effort to understand the data at a high-level.&lt;/p&gt;

&lt;p&gt;Is that split workflow the most efficient way of doing things? Is there tool that might provide a streamlined analysis, both providing a way to understand the high-level, as well as offering interactive apps for entire departments? While the tools mentioned above are certainly fantastic, we feel that &lt;strong&gt;R&lt;/strong&gt; could help make life a lot easier for BI professionals.&lt;/p&gt;

&lt;p&gt;While we don’t want to oversell it’s abilities, think of how often analysts turn to the above tools to do things like&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Understanding how data is distrubted&lt;/li&gt;
  &lt;li&gt;Finding how particular columns are correlated&lt;/li&gt;
  &lt;li&gt;Offering pivot tables&lt;/li&gt;
  &lt;li&gt;Making histograms or scatterplots&lt;/li&gt;
  &lt;li&gt;Grouping by a column of interest and plotting a trend&lt;/li&gt;
  &lt;li&gt;Calculating statistics (like standard deviations, t-tests, quantiles)&lt;/li&gt;
  &lt;li&gt;Creating interactive visualizations for others&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You may be surprised to hear that R can also do those things, and do them &lt;em&gt;well&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;If you’re using Excel for things like financial modeling, and/or have the need to input data frequently, then moving to R won’t make sense. We’ll be the first to say that Excel can be a super effective tool.&lt;/p&gt;

&lt;p&gt;But, if you’re often doing analysis using the tools mentioned above, we’re excited to help you see what R can do. Besides the above, here are other benefits of R compared to Excel/Qlik&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Lends itself to source control&lt;/li&gt;
  &lt;li&gt;Makes your work easily reproducible&lt;/li&gt;
  &lt;li&gt;Lets you tell a data story (combining process and presentation in a notebook)&lt;/li&gt;
  &lt;li&gt;Allows one to more easily find and fix errors&lt;/li&gt;
  &lt;li&gt;Easy to work on very large datasets&lt;/li&gt;
  &lt;li&gt;Offers machine learning&lt;/li&gt;
  &lt;li&gt;It’s free&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We’ll try to accomplish this in a series of blog posts. Each blog post will contain an R notebook, that’ll have explanations, R code, and R plots to help you get stated. &lt;a href=&quot;http://healthcare.ai/notebooks/IntroHealthDataAnalysisInR.nb.html&quot;&gt;Here’s the first notebook in this series&lt;/a&gt;. Enjoy!&lt;/p&gt;</content><author><name>Levi Thatcher</name></author><category term="analysis" /><summary type="html">When working with data in healthcare, business intelligence (BI) folks often turn to tools like Excel, SSMS, Tableau, and Qlik. Typically, multiple tools will be used when analyzing a dataset. Sometimes the analyst will use Excel to look at the data, get a sense for how the column are distributed, perhaps make a histogram or scatterplot. Often, analysts will later turn to Qlik and/or Tableau to provide an interactive app, often hosted on a dedicated server, such that folks in other departments can explore the same data. In this same process, the analyst or data architect may start by querying the database in SSMS as well, to do some simple counts and group by’s, in an effort to understand the data at a high-level.</summary></entry><entry><title type="html">Contributing to open source software development using Github</title><link href="http://healthcare.ai/blog/2017/01/12/open-source-and-git/" rel="alternate" type="text/html" title="Contributing to open source software development using Github" /><published>2017-01-12T01:32:11-07:00</published><updated>2017-01-12T01:32:11-07:00</updated><id>http://healthcare.ai/blog/2017/01/12/open-source-and-git</id><content type="html" xml:base="http://healthcare.ai/blog/2017/01/12/open-source-and-git/">&lt;p&gt;The purpose of this post is to help you become familiar with &lt;em&gt;Git&lt;/em&gt;, an essential part of contributing to &lt;a href=&quot;www.healthcare.ai&quot;&gt;healthcare.ai&lt;/a&gt;. Git is essentially a collaboration tool for software developers, and &lt;a href=&quot;www.github.com&quot;&gt;&lt;em&gt;Github&lt;/em&gt;&lt;/a&gt; is the accompanying online storage platform. If you have been reading about healthcare.ai, you probably know that it is an &lt;a href=&quot;http://healthcare.ai/#why&quot;&gt;&lt;em&gt;open source&lt;/em&gt; software package&lt;/a&gt;. Open source means that we aren’t hiding anything from our users. They can use the package, view the contents, and modify the package for their particular needs. We chose to make healthcare.ai open source for two major reasons:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Anyone can contribute to its development.&lt;/strong&gt; When an open source package becomes popular, there could be hundreds of people working on making it better!&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;The rising tide will raise all boats.&lt;/strong&gt; We are trying to be a leader in healthcare machine learning, and hoping that our efforts will be visible, benefit the community, and benefit patient care.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As our team (and number of contributors) grows, it doesn’t make sense to share our code using email attachments. We make thousands of changes on hundreds of files. Instead, we use Git. Git is an industry standard service that makes it easy for large teams to collaborate on code, keep files safe from unwanted changes through &lt;em&gt;version control&lt;/em&gt;, and facilitate code review before changes are published.&lt;/p&gt;

&lt;p&gt;The idea of Git is pretty easy to follow, but the vocabulary can be a little confusing at first. There are &lt;a href=&quot;https://guides.github.com/introduction/flow/&quot;&gt;many&lt;/a&gt; &lt;a href=&quot;http://git.huit.harvard.edu/guide/&quot;&gt;great&lt;/a&gt; &lt;a href=&quot;https://guides.github.com/activities/hello-world/&quot;&gt;resources&lt;/a&gt; about Git online, but sometimes they are too basic for specific issues, or too complicated and lacking explanation. Hopefully you find these ideas basic but informative.&lt;/p&gt;

&lt;p&gt;Imagine that you are writing a paper. You write some, save the file: “myPaper_version1.doc.” You write more, save version 2, version 3… Essentially, this is what Git is doing. The codebase, or repository, is written line by line, and each change can be stored as a &lt;em&gt;commit&lt;/em&gt;. The commits flow linearly, and if you don’t like the latest changes, you can roll back to the previous commit. This &lt;a href=&quot;http://rogerdudler.github.io/git-guide/files/git_cheat_sheet.pdf&quot;&gt;excellent cheat sheet shows that process&lt;/a&gt;, where each circle represents a commit.&lt;/p&gt;

&lt;p&gt;I’ll focus on explaining 5 topics that are likely to help you collaborate.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Local vs Source&lt;br /&gt;
A code repository lives at &lt;a href=&quot;www.github.com&quot;&gt;www.github.com&lt;/a&gt;, and you’d like to work on it. The &lt;em&gt;master&lt;/em&gt; source is online. You need to &lt;em&gt;clone&lt;/em&gt; or &lt;em&gt;fork&lt;/em&gt; the repo to create a local copy on your computer. You can make improvements, commit them, and document each change. Verify that your code is working, then, they can be &lt;em&gt;merged&lt;/em&gt; into the source copy.&lt;/li&gt;
  &lt;li&gt;Pushing and Pulling&lt;br /&gt;
When you are ready to submit your changes to the master, you must first &lt;em&gt;pull&lt;/em&gt; changes from the master source to your local copy. This ensures that you have the latest changes from the master in your local copy. Then you can &lt;em&gt;push&lt;/em&gt; your changes up to the online master. If you take a few weeks away from the code, make sure you pull the latest changes into your local copy before starting your work.
    &lt;ul&gt;
      &lt;li&gt;Pull changes from the master source to your local copy: &lt;code class=&quot;highlighter-rouge&quot;&gt;git pull&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Push your local changes to the online master: &lt;code class=&quot;highlighter-rouge&quot;&gt;git push&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Branches&lt;br /&gt;
&lt;a href=&quot;https://github.com/HealthCatalystSLC/healthcareai-r/branches&quot;&gt;&lt;em&gt;Branches&lt;/em&gt;&lt;/a&gt; are used to help keep large changes, feature additions, etc. separate from the master source. They allow you to “break the code to fix it” without worrying that you are going to ruin the master. If you create a topic branch to work in, you now have two separate local copies: your local master, and your local branch. You can make ongoing changes to the code in your branch, then switch back to the master to actually use code that you know is working.
    &lt;ul&gt;
      &lt;li&gt;Create a new topic branch or switch to an existing topic branch: &lt;code class=&quot;highlighter-rouge&quot;&gt;git checkout -b nameofbranch&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Merging&lt;br /&gt;
When the topic branch you’ve been developing is done, documented, and functional, it’s ready to be merged back into the master branch. Again, update your local master, as other people could have been working on the code while you were. Merge the local master into your local branch (the ordering can be disputed, but this is how we do it at healthcare.ai), allowing you to test for functionality with all the latest changes. Finally, push the local branch up to the server and ask for review.
    &lt;ul&gt;
      &lt;li&gt;Check out the latest online master and merge into the local topic branch: &lt;code class=&quot;highlighter-rouge&quot;&gt;git merge origin/master&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Make sure it all still works!&lt;/li&gt;
      &lt;li&gt;Push the local branch to the server for review: &lt;code class=&quot;highlighter-rouge&quot;&gt;git push&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Pull Requests&lt;br /&gt;
After pushing a completed topic branch up to the server, the &lt;em&gt;pull request&lt;/em&gt; acts as a request for code review. The term pull request &lt;del&gt;makes no sense to me&lt;/del&gt; &lt;a href=&quot;https://www.quora.com/GitHub-Why-is-the-pull-request-called-pull-request&quot;&gt;Got it!&lt;/a&gt; Another developer will look through your changes and documentation, ask for revisions, and eventually approve the pull request. After this happens, the branch will be merged into the online master. Anyone who clones the master will now get your changes, and the branch can be deleted.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There is a lot of vocabulary in this post, but we hope that it’s helpful to see these things defined in context of how they are used. Seasoned developers forget that Git language can be hard to interpret since it’s such an important part of their everyday work. The truth is, it’s a barrier to entry for the casual contributor. However, it’s extremely important for the health of a large code project and the team that works on it.&lt;/p&gt;

&lt;p&gt;If you’ve become motivated to get down to business, make an account on Github [www.github.com], &lt;a href=&quot;https://github.com/HealthCatalystSLC/healthcareai-r/blob/master/CONTRIBUTING.md#clone-healthcareai-r-repo&quot;&gt;clone our repo&lt;/a&gt;, and help us shape the future of healthcare machine learning! There are more detailed instructions in the &lt;a href=&quot;https://github.com/HealthCatalystSLC/healthcareai-r/blob/master/README.md&quot;&gt;readme&lt;/a&gt; and &lt;a href=&quot;https://github.com/HealthCatalystSLC/healthcareai-r/blob/master/CONTRIBUTING.md&quot;&gt;contributing&lt;/a&gt; files, and you can feel free to &lt;a href=&quot;http://healthcare.ai/contact.html&quot;&gt;send questions&lt;/a&gt; our way.&lt;/p&gt;</content><author><name>Mike Mastanduno</name></author><category term="workflow" /><summary type="html">This blog will describe the motivation and workflow of Git version control</summary></entry><entry><title type="html">Know your business question: A focus on readmissions</title><link href="http://healthcare.ai/blog/2017/01/11/know-your-business-question-a-focus-on-readmissions/" rel="alternate" type="text/html" title="Know your business question: A focus on readmissions" /><published>2017-01-11T21:00:00-07:00</published><updated>2017-01-11T21:00:00-07:00</updated><id>http://healthcare.ai/blog/2017/01/11/know-your-business-question-a-focus-on-readmissions</id><content type="html" xml:base="http://healthcare.ai/blog/2017/01/11/know-your-business-question-a-focus-on-readmissions/">&lt;p&gt;As time goes on, we will not only discuss healthcare machine learning (ML) and health in the US at a high level, but also specific ways ML might help drive outcomes improvements. Many health systems are working on reducing their readmission rate—which is often considered a measure of quality of care and can be tied to penalties. For  hospital systems progressing toward ML for readmissions—or any measure—the first step is to identify your most important business questions; the next step is creating a suitable dataset to create the model. There are often several points where business logic dictates decisions related to the dataset and whether one or multiple models are needed to help with a specific process. That’s certainly the case when creating readmission risk models.&lt;/p&gt;

&lt;p&gt;Readmission risk models improve patient quality of life and decrease mortality by providing extra care or surveillance to high-risk patients. Implementing a readmission risk model could serve two different purposes:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Identifying high-risk &lt;em&gt;observational&lt;/em&gt; and &lt;em&gt;inpatient&lt;/em&gt; patients as soon after their admission as possible to answer this question: &lt;em&gt;Which in-hospital patients are most at risk for readmission?&lt;/em&gt; By answering this question, doctors, nurses, and in-hospital staff can intervene to try to lower a patient’s readmission risk.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Identifying high-risk &lt;em&gt;discharged&lt;/em&gt; patients as soon after their discharge as possible to answer this question: &lt;em&gt;Which discharged patients are most at risk of readmission?&lt;/em&gt; By answering this question, hospital support staff and transitional services can intervene to try to lower a patient’s readmission risk.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now, you might think, “Those are the same use case: they both predict readmissions, even sometimes for the same patients.” But remember, an ML model is only valuable if it provides actionable insight. Nurses are in the optimal position to help when the patient is in the hospital. Furthermore, as we &lt;a href=&quot;http://healthcare.ai/blog/2017/01/06/data-leakage-in-healthcare-machine-learning/&quot;&gt;discussed at length&lt;/a&gt; last week, the data sources for these two use cases are very different. The type and amount of information available at discharge is different from what is available at admission. The best ML models are built custom to a specific dataset and use case. One risk model is not sufficient to answer both questions, and accuracy would almost certainly be compromised if they were naively combined. Though similar, these questions need models that relate to different types of patients and are targeted toward different interventions. Under the hood of the models, we need to use different datasets (to avoid &lt;a href=&quot;http://healthcare.ai/blog/2017/01/06/data-leakage-in-healthcare-machine-learning/&quot;&gt;data leakage&lt;/a&gt;)—and maybe even different algorithms. The best ML model will always be built to answer a specific question, tailored to specific data, and targeted toward the most effective intervention.&lt;/p&gt;

&lt;p&gt;OK, so we know we need two models for readmission risk modeling. But to complicate things even more, we must keep in mind that the layman definition of readmission and the &lt;a href=&quot;https://www.cms.gov/Medicare/Medicare-Fee-for-Service-Payment/PhysicianFeedbackProgram/Downloads/2014-ACR-MIF.pdf&quot;&gt;CMS definition of readmission&lt;/a&gt; are also subtly different. We must base our outcome variable on a definition very similar to that of CMS because it is consistent with the way many hospital systems track and report outcomes, and the CMS definition of readmissions is ultimately the measure hospital systems are trying to affect. Knowing that definition is critical to building a model to address it.&lt;/p&gt;

&lt;p&gt;Per the CMS definition, patients in the hospital can be &lt;em&gt;emergency department&lt;/em&gt;, &lt;em&gt;observational&lt;/em&gt;, or &lt;em&gt;inpatient&lt;/em&gt;. To be considered an unplanned readmission patients must initially be discharged from the inpatient setting. A patient that is discharged from the emergency department or observation setting cannot be readmitted (0% probability) because they did not meet the inpatient requirement pertaining to the inpatient index admission. Similarly, even after a patient is admitted to the inpatient setting, we still do not yet know their what their discharge disposition or discharge diagnosis will be. If the patient leaves against medical advice or is assigned a cancer-related discharge, for instance, they meet a different set of exclusion criteria and cannot be readmitted (again, 0% probability). While the specific criteria behind the definition of a readmission makes practical sense, it creates a couple of challenges to training, testing, and deploying a readmission risk model that is to be leveraged while patients are still in the hospital.&lt;/p&gt;

&lt;p&gt;At the end of the day, we need to develop the model using the same data that we want it to perform well on in production. For the &lt;em&gt;in-hospital&lt;/em&gt; use case, &lt;em&gt;observational&lt;/em&gt; and &lt;em&gt;emergency department&lt;/em&gt; patients that would ultimately be excluded at discharge must also be excluded from model development. These patients may skew the model toward predicting 0% readmission probability since they are guaranteed to be 0% risk as defined by CMS. This puts us in a tight spot. When &lt;a href=&quot;http://healthcare.ai/blog/2016/12/15/model-evaluation-using-roc-curves/&quot;&gt;evaluating the model&lt;/a&gt;, it may appear to have higher accuracy by skewing toward low probabilities because it was improperly trained on data that should have been excluded. For the post-discharge use case, the discharge type is available and the model can match the CMS definition more closely. This will likely lead to increased accuracy overall, as more use-case specific data is available.&lt;/p&gt;

&lt;p&gt;From our experience, understanding the definition of the readmission outcome variable, the specific use case, and the timing/target is crucial. There is clearly a trade-off between timeliness and accuracy, and to have the greatest impact on patient outcomes, it is important to develop readmission risk models based on data that reflects the use case in production. Again, the best ML model should answer a specific question, using specific data, with an actionable result. Keep these things in mind and your models will improve.&lt;/p&gt;

&lt;p&gt;Thanks for reading and please &lt;a href=&quot;http://healthcare.ai/contact&quot;&gt;reach out&lt;/a&gt; with any questions or comments!&lt;/p&gt;</content><author><name>Taylor Larsen</name></author><category term="overview" /><summary type="html">This post describes the importance of understanding the business questions, use cases, and data when creating a readmission risk model</summary></entry><entry><title type="html">Which regions of the US are healthy?</title><link href="http://healthcare.ai/blog/2017/01/08/us-health-by-county/" rel="alternate" type="text/html" title="Which regions of the US are healthy?" /><published>2017-01-08T16:00:00-07:00</published><updated>2017-01-08T16:00:00-07:00</updated><id>http://healthcare.ai/blog/2017/01/08/us-health-by-county</id><content type="html" xml:base="http://healthcare.ai/blog/2017/01/08/us-health-by-county/">&lt;p&gt;While our previous posts have focused on healthcare machine learning, we’re also excited to post analyses of health data using R and Python. We do this to hopefully elevate the national discussion around health data, enhance the community’s understanding of health in the United States (US), and provide guidance as to how communities and health systems might increase the quality and length of people’s lives. Health Catalyst is an outcomes improvement company, and we realize that the inpatient setting is only one of several venues that affect a person’s health trajectory. Understanding the big picture of health is another way to approach outcomes improvements. These posts will not only attempt to educate on findings about health, but also on how to use R/Python for health data analysis, so we’ll always post links to the &lt;a href=&quot;https://gist.github.com/levithatcher/070496ca48c165d7ced37e0ffcd24dc7&quot;&gt;relevant code&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Those who have been in healthcare for any significant amount of time have heard that social determinants of health (SDOH) are important to healthcare outcomes. While it’s hard to overstate the importance of these factors, they’re often not well understood and overshadowed by inpatient optimizations when discussing outcomes improvement. This post is the first in a series where we’ll attempt to untangle the drivers behind population health differences across the US. Today we’ll talk about where the US stands from region to region in terms of social determinants of health. In subsequent posts we’ll discuss whether these differences mostly related to income, air pollution, access to healthy food, long commutes, access to healthcare, opioid addiction, or alcohol abuse.&lt;/p&gt;

&lt;p&gt;To try and answer that we’ll &lt;a href=&quot;https://gist.github.com/levithatcher/070496ca48c165d7ced37e0ffcd24dc7&quot;&gt;use R&lt;/a&gt; and &lt;a href=&quot;http://www.countyhealthrankings.org/sites/default/files/2015%20CHR%20Analytic%20Data.csv&quot;&gt;data&lt;/a&gt; from &lt;a href=&quot;http://www.countyhealthrankings.org/&quot;&gt;countyhealthrankings.org&lt;/a&gt;, which is a fantastic resource on SDOH comparisons by county. We’ll start by presenting a choropleth map of median household 2015 &lt;a href=&quot;http://www.countyhealthrankings.org/measure/median-household-income&quot;&gt;income by county&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/Post10CountyHealthOverview/MedianIncomeByCounty.jpg&quot; alt=&quot;IncomeByCounty&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Of course, what we find is that there are large regional differences in household income. Broadly, the Northeast, the West Coast, and metropolitan areas are associated with higher personal incomes, compared with rural areas and the South. Note that occasionally there is high intra-state variation, such as in Texas, compared to the more uniform median incomes across counties in Minnesota. But how do these regional variations in incomes correspond with healthcare outcomes? Data on &lt;a href=&quot;http://www.countyhealthrankings.org/measure/low-birthweight&quot;&gt;Low Birth-Weight&lt;/a&gt; (LBW), i.e., live births under ~5 lbs 8 oz (2500 g), provides a &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/7633862&quot;&gt;helpful link&lt;/a&gt; between social determinants and actual healthcare outcomes, as&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“&lt;a href=&quot;http://www.countyhealthrankings.org/measure/low-birthweight&quot;&gt;LBW indicates maternal exposure to health risks&lt;/a&gt; in all categories of health factors, including her health behaviors, access to health care the social and economic environment she inhabits, and environmental risks to which she is exposed. In terms of the infant’s health outcomes, LBW serves as a predictor of premature mortality and/or morbidity over the life course and for potential cognitive development problems.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Pulling data from &lt;a href=&quot;http://www.countyhealthrankings.org/measure/low-birthweight&quot;&gt;here&lt;/a&gt;, and &lt;a href=&quot;https://gist.github.com/levithatcher/070496ca48c165d7ced37e0ffcd24dc7&quot;&gt;processing with R&lt;/a&gt;, we plot the percentage of county live births with birth-weight under 5 lbs 8 oz:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/Post10CountyHealthOverview/LowBirthWeightByCounty.jpg&quot; alt=&quot;LBWByCounty&quot; /&gt;&lt;/p&gt;

&lt;p&gt;While there’s a lot that could be unpacked here, we’ll simply note that the same regions that had lower personal incomes also have a higher percentage of LBW. Not a huge surprise—it is surprising, however, how much intra-state variation is present (like in NV and CO) and how the Deep South has rates of LBW that are often twice that of Minnesota and Wisconsin.&lt;/p&gt;

&lt;p&gt;While income appears to be associated with new-born morbidity and mortality, how does it affect populations later in life? We use &lt;a href=&quot;http://www.countyhealthrankings.org/measure/premature-death-ypll&quot;&gt;premature mortality&lt;/a&gt; &lt;a href=&quot;http://www.countyhealthrankings.org/sites/default/files/2015%20CHR%20Analytic%20Data.csv&quot;&gt;data&lt;/a&gt; from &lt;a href=&quot;http://www.countyhealthrankings.org/&quot;&gt;countyhealthrankings.org&lt;/a&gt;, where a premature death is defined as occurring before 70 years of age, to answer this question. For each county, per 100k people, the years of death before 75 are summed. Think of it as incidence of early death, per county:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/Post10CountyHealthOverview/PrematureDeathByCounty.jpg&quot; alt=&quot;PrematureDeathByCounty&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Compared to LBW, it appears that premature mortality more closely corresponds with median county income. Note how the high incidence of premature mortality across Arkansas, Tennessee, and Kentucky closely tracks income (comparing with the first figure). Broadly, Appalachia appears to suffer more from deaths of prime-age adults compared to LBW (while the South appears to suffer greatly from both). Note that while the rust belt (i.e., PA, OH, IN, MI, IL, and WI) certainly has other issues, they seem to be doing a good job of keeping prime-age people alive, especially compared to Appalachia and the Deep South.&lt;/p&gt;

&lt;p&gt;While it’s old-hat to say that population health in the Deep South isn’t fantastic, let’s go one step further and see which counties in the US punch above their weight when it comes to using resources effectively. In other words, which counties are doing well for how poor they are. This is the metric that will let us understand what it is that poor and middle-income counties do well in terms of keeping people healthy.&lt;/p&gt;

&lt;p&gt;To get at this, we create percentiles for each county in terms of LBW (where 100 is best) and then subtract (for each county) the percentiles for income. We can call this the county Punch-Above-Their-Weight Index or PATWI for short. Before plotting the entire country, let’s look at LBW PATWI for the top ten counties in terms of income:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/Post10CountyHealthOverview/TableHighestIncomeAndLBW.png&quot; alt=&quot;TableHighestIncomeandLBW&quot; /&gt;&lt;/p&gt;

&lt;p&gt;First, in this and the following tables, the PATWI column is just the fifth column minus the fourth column. While the counties above do have good scores in terms of LBW, it’s difficult for rich counties to punch above their weight, since they’re the 800-lb gorillas. We do, however, see the benefit of the PATWI, since the richest counties in the US &lt;em&gt;aren’t&lt;/em&gt; those with the best LBW scores.&lt;/p&gt;

&lt;p&gt;Which counties have the best LBW, considering their income?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/Post10CountyHealthOverview/TableHighestLBWPATWIAndIncome.png&quot; alt=&quot;TableHighestPATWIAndIncome&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It’s impressive that these counties, which have median incomes one-third of that of Loudoun County (VA), have better LBW scores than Loudoun County. Note that the best LBW PATWI scores come from a scattered grouping of states, although, Michigan (MI) impressively has three entries and Missouri (MO) has two. That’s definitely good news for MI and MO public health officials. Note that this PATWI measure doesn’t just bias towards the poorest counties, either. While Buffalo County, SD—perhaps the poorest county in the nation—has a high LDW PATWI score, none of the counties from the Deep South or Appalachia show up in this list. Here’s LDW PATWI score for all counties:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/Post10CountyHealthOverview/LBWComparedToIncomeByCounty.jpg&quot; alt=&quot;LBWComparedToIncome&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that positive values here denote that the county is doing better at LBW than expected, based on their income*. Amongst other findings, we see that even though northern half of West Virginia is quite poor, they’re doing better than expected at helping mothers carry and deliver healthy babies. The Deep South, however, is doing about as poorly (or worse) than one would expect by looking at their income along. Note that Missouri is doing better than expected, as is rural Oregon, Minneapolis, and Wisconsin. Recall that metro areas tend to be richer than rural areas, so it’s impossible for richest areas like the Bay Area and NYC tend to have a high score (since their percentiles can’t go over 100)—this limitation makes this plot more of a measure of how middle and low income counties are doing relative to their income.
Let’s do the same for the premature mortality metric we discussed above—in other words, which counties are doing a great job of avoiding early mortality. We’ll start with the richest counties:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/Post10CountyHealthOverview/TableHighestIncomeAndPrematureDeath.png&quot; alt=&quot;TableHighestIncomeAndLBW&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Yup, the richest counties are great at keeping people alive—even more so than avoiding low birth-weight (compare with the first table above). But, which poor or middle income counties are best at punching above their weight when it comes to keeping their citizens alive?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/Post10CountyHealthOverview/TableHighestPrematureDeathPATWIAndIncome.png&quot; alt=&quot;TableHighestLBWPATWIAndIncome&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It’s impressive how these relatively poor counties are achieving the top 10-20th percentile in terms of avoiding premature deaths (note that we’re using higher percentiles to mean good outcomes). It is an interesting mix of counties, indeed. Santa Cruz (AZ), Presidio County (TX), and Maverick County (TX) border Mexico; Crowley County (CO) has the largest per-capita prison population in the country. Madison County, home to BYU-Idaho, is &lt;a href=&quot;http://www.slate.com/articles/life/map_of_the_week/2012/02/mormon_population_in_the_u_s_an_interactive_map.html&quot;&gt;~80% Mormon&lt;/a&gt;, which likely means that county overall has low rates of alcohol, tobacco, and drug dependency. In a future post we’ll go into how certain counties are punching above their weight, and for now offer the national view of premature-death PATWI score:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/Post10CountyHealthOverview/PrematureDeathComparedToIncomeByCounty.jpg&quot; alt=&quot;PrematureDeathComparedToIncome&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Recall that darker means that the county is doing well. At this level, what we notice is that Wisconsin, Michigan, and Missouri are doing a good job keeping prime-age people alive compared to Nevada and Wyoming. Knowing why this is occurring can significantly change how a health system interacts with its patients.&lt;/p&gt;

&lt;p&gt;At Health Catalyst, we strive to understand the significant way social determinants of health can affect decision making at health systems from the Deep South to the Bay Area. Our clients are located all across the continent. High-value improvements in one health system are not necessarily the best everywhere, and we’d be cheating ourselves to only look for improvements at inpatient units. We’re committed to leveraging the tools of population health to improve patient outcomes across the board. 
This post is the first of many on population health, and in a future post we’ll dig more into which social determinants are most driving regional variations of LBW and premature death.&lt;/p&gt;

&lt;p&gt;Thanks for reading and please &lt;a href=&quot;http://healthcare.ai/contact&quot;&gt;reach out&lt;/a&gt; with any questions or comments!&lt;/p&gt;

&lt;p&gt;* The distribution of county median income has a long tail, which means that when subtracting county income percentiles from the corresponding LBW or premature death percentiles, the result is typically positive.&lt;/p&gt;</content><author><name>Levi Thatcher</name></author><category term="overview" /><summary type="html">While our previous posts have focused on healthcare machine learning, we’re also excited to post analyses of health data using R and Python. We do this to hopefully elevate the national discussion around health data, enhance the community’s understanding of health in the United States (US), and provide guidance as to how communities and health systems might increase the quality and length of people’s lives. Health Catalyst is an outcomes improvement company, and we realize that the inpatient setting is only one of several venues that affect a person’s health trajectory. Understanding the big picture of health is another way to approach outcomes improvements. These posts will not only attempt to educate on findings about health, but also on how to use R/Python for health data analysis, so we’ll always post links to the relevant code.</summary></entry><entry><title type="html">Data leakage in healthcare machine learning</title><link href="http://healthcare.ai/blog/2017/01/06/data-leakage-in-healthcare-machine-learning/" rel="alternate" type="text/html" title="Data leakage in healthcare machine learning" /><published>2017-01-06T15:00:00-07:00</published><updated>2017-01-06T15:00:00-07:00</updated><id>http://healthcare.ai/blog/2017/01/06/data-leakage-in-healthcare-machine-learning</id><content type="html" xml:base="http://healthcare.ai/blog/2017/01/06/data-leakage-in-healthcare-machine-learning/">&lt;p&gt;Note: this is a technical post.&lt;/p&gt;

&lt;p&gt;To leverage lessons learned during our model building engagements here at Health Catalyst, let’s explore the &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.365.7769&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;subject of data leakage&lt;/a&gt;. Data leakage occurs when a predictive model is trained using information that is available in training data but not actually available for predicting outcomes in production. Models with data leakage tend to be very accurate in development, but perform poorly in production, where they are ultimately used.&lt;/p&gt;

&lt;p&gt;More specifically, leakage in the context of healthcare machine learning occurs when:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A feature is used to train the model that would &lt;em&gt;not be available in production at the time of prediction&lt;/em&gt;. An example of this might be using the number of oral medications a patient is currently taking to predict length of stay at admission when medication reconciliation may not take place for up to 24 hours following admission.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A feature is used to train the model that would &lt;em&gt;not be available in production prior to the outcome variable being populated&lt;/em&gt;. An example of this might be using a response from a quality of life phone survey to predict readmissions when the survey is not administered until three months after the discharge. At that point, it would already be known whether the patient had been readmitted or not. When training a model, the algorithm has no idea whether a feature was populated prior to the target variable in the same row.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A feature is used to train the model that is &lt;em&gt;outside the scope of the model’s intended use case&lt;/em&gt;. An example of this might exist when trying to predict the probability of a patient having heart failure and using the hospital unit associated with the patient without considering that the hospital unit may be disease or service specific (like a cardiac unit).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;em&gt;correct outcome is leaked into the test data&lt;/em&gt; through a variable that inherently proxies for the outcome. An example of this might be predicting the probability that a patient will pay their balance due on time and using a variable that indicates whether the patient has been contacted by the hospital accounts receivable department which only takes place when a patient is late on payment.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;What can happen?&lt;/em&gt; Leakage can lead to poor generalization, overfitting, and over-estimation of a model’s performance. The ultimate negative impact of leakage is the deployment of a less useful model than if no leakage was present. Considering the examples described above, leakage can result in the inclusion of a variable that appears predictive during training, but due to missing data and/or imputation in production, the variable is either not predictive, is skewed in power, or only appropriate in certain use cases. Leakage will often raise the accuracy of a model in training, but make predictions that can’t be trusted in deployment.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;How does one prevent it?&lt;/em&gt; Through proactive analysis of potentially predictive variables and direct involvement of subject matter experts (SMEs) during variable selection, leakage can most likely be avoided. It is important to profile each variable to determine when it was generated and how its values are distributed when comparing training data to production data. Involving clinical, operational, and data SMEs will reduce the likelihood of leakage by improving understanding of nuances in the data. It is also important to understand when the prediction needs to take place so that predictive variables can be sourced accordingly; and the timing of the prediction should be based on the use case for the predictive model output. In summary, one must scrutinize the data they are using and keep the business question in mind when building the model.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Does an existing model have data leakage?&lt;/em&gt; As described above, profiling each variable and consulting with SMEs may help to identify more obvious leakage. Another way of identifying leakage is to compare the model’s actual performance in production to the model’s performance observed during training and testing. This can expose important discrepancies that might be the result of leakage and in depth analysis comparing training data to production data may be required.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;How does one fix an existing model?&lt;/em&gt; If leakage is identified after a model has been developed and/or deployed, it is important to remediate the issue. Redefining a variable and retraining the model can eliminate leakage and allow the variable to remain in the model. Another solution for eliminating leakage is to remove the variable and retrain the model, while exploring other leakage free variables and proxies that could be added. Though perceived model performance might take a hit when data leakage is avoided/eliminated, the predictions on new data in production will be more accurate and useful.&lt;/p&gt;

&lt;p&gt;Clearly, leakage is an issue that we face on a regular basis, especially when dealing with all the complexities associated with healthcare data. Through experience and diligent analysis, it is an issue we can easily avoid. &lt;a href=&quot;http://healthcare.ai/&quot;&gt;Healthcare.ai&lt;/a&gt; has several tools to help understand the timing, scope, and source of variables when model building, which can be used to eliminate data leakage during the model’s initial creation. Taking these steps will mean deploying useful models that are widely adopted, and ultimately improve healthcare outcomes.&lt;/p&gt;

&lt;p&gt;Thanks for reading and please &lt;a href=&quot;http://healthcare.ai/contact&quot;&gt;reach out&lt;/a&gt; with any questions or comments!&lt;/p&gt;</content><author><name>Taylor Larsen</name></author><category term="overview" /><summary type="html">This blog will describe data leakage along with its causes, impacts, and fixes in the context of healthcare machine learning</summary></entry><entry><title type="html">Applications of healthcare machine learning</title><link href="http://healthcare.ai/blog/2016/12/22/applications-of-healthcare-machine-learning/" rel="alternate" type="text/html" title="Applications of healthcare machine learning" /><published>2016-12-22T21:39:11-07:00</published><updated>2016-12-22T21:39:11-07:00</updated><id>http://healthcare.ai/blog/2016/12/22/applications-of-healthcare-machine-learning</id><content type="html" xml:base="http://healthcare.ai/blog/2016/12/22/applications-of-healthcare-machine-learning/">&lt;p&gt;Now that we have been through some of the applications of machine learning (ML) in mainstream technology, we thought it would be nice to give a broader overview of some of the different types of ML and how they might be applied to improve patient care. &lt;a href=&quot;http://healthcare.ai/blog/2016/12/21/which-algorithms-are-in-healthcareai/&quot;&gt;We explored the algorithms&lt;/a&gt; that currently make up healthcare.ai, and alluded to the fact that there is lots of room for expansion. We’ll take this post as an opportunity to speculate on where healthcare ML could go in the near and distant future. Along the way, we’ll discuss the different types of ML algorithms and give examples of their use in healthcare. 
At its most basic definition, machine learning refers to a group of algorithms that learn from data. These algorithms are different from &lt;a href=&quot;https://fiftyexamples.readthedocs.io/en/latest/celsius.html&quot;&gt;conventional ones&lt;/a&gt; since they work using examples rather than rules. If you went to the hospital for flu like symptoms, a doctor thinking along the lines of traditional algorithms might say, “You have a fever, aches, general weakness, and no cold symptoms. This looks like the flu.” A different doctor, thinking like an ML algorithm, would say, “Hmmm, your symptoms are the same as 50 recent patients who had the flu. You probably do too.” Silly example, but it’s worth noting a couple of things. First, the ML doctor doesn’t actually need to know anything about the flu before they start making diagnoses. Second, their diagnoses probably won’t be very good until they have seen a lot of examples to compare against.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Classification vs. Regression&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The above scenario is an example of a classification machine learning problem. A classification algorithm will give a probability score of a person having the disease, or, more broadly, the probability of event happening vs. not. Healthcare.ai has already implemented some of the simplest &lt;a href=&quot;http://healthcare.ai/blog/2016/12/21/which-algorithms-are-in-healthcareai/&quot;&gt;algorithms&lt;/a&gt; to answer questions like:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;What is the likelihood that a patient will develop a central line infection?&lt;/li&gt;
  &lt;li&gt;What is the likelihood that a COPD patient will be readmitted within 90 days of discharge?&lt;/li&gt;
  &lt;li&gt;What is the likelihood that a person will no-show for their appointment?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These questions are posed with a lot of example data and the expectation that a model will give a probability from 0 (low) to 1 (high). It’s up to us to draw the line of what we call a positive prediction and what we call a negative prediction. On the other hand, a regression algorithm will predict a continuous value. Here are some examples of questions that we have or plan to tackle using regression algorithms:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;How many days will a patient need to stay in the hospital?&lt;/li&gt;
  &lt;li&gt;How many people do we need on staff in the ED on a given night?&lt;/li&gt;
  &lt;li&gt;How much money will a patient cost the health system over the next year?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There are a lot of exciting questions that can be answered with very basic machine learning! As we’ve said before, we are focused on trying to answer the questions that will make the most impact right now. Luckily for us, there is still a lot of low-hanging fruit in healthcare for our team to address. As long as there is good example data, ML could help answer a huge range of questions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Supervised vs. Unsupervised&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;All the problems that this post has discussed so far are supervised machine learning problems. For each of these there is a ground truth associated with every patient example being used to train the model. The other major type of machine learning is unsupervised. There is no ground truth associated with the data. The algorithms in this category are largely related to identifying patterns and similarity, and using them to group or stratify data into different categories. This type of functionality is a high priority capability that we are working on implementing in healthcare.ai. In the very near future, we hope to be able to use clustering methods and anomaly detection to answer questions like:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Does this patient (who hasn’t been associated with diabetes) belong in a diabetes registry? Or a heart disease registry? And with what likelihood?&lt;/li&gt;
  &lt;li&gt;How similar are my high-utilizing patients? Do they fall into particular clusters? What can we learn about the characteristics of these separate clusters?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These questions are typically more nebulous than supervised learning problems, but useful insights can still be gathered. For example, there would be value in labeling a non-diabetic patient as a person to watch and intervene before they ever develop diabetes. This is great information to have and ML will absolutely make an impact by answering such questions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Future&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The third arm of machine learning that has especially gotten a lot of attention lately is in Artificial Intelligence (AI), mostly implemented with what’s called deep learning. &lt;a href=&quot;https://www.tesla.com/autopilot&quot;&gt;Self-driving cars&lt;/a&gt;, &lt;a href=&quot;https://www.facebook.com/notes/mark-zuckerberg/building-jarvis/10154361492931634/&quot;&gt;home assistants&lt;/a&gt;, and &lt;a href=&quot;http://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html?smid=pl-share&amp;amp;_r=0&quot;&gt;translation services&lt;/a&gt; are some applications of AI. This is the cutting edge in ML right now. In the 2-5 year timeframe, we will start to see more mainstream impact from these techniques. In healthcare, the first big use case is for image analysis in radiology and pathology departments. It’s possible that computers will learn to assess images with high speed and accuracy in the very near future. When the community is ready for adoption, we will be excited to provide these tools in healthcare.ai.
Hopefully this post helped you to understand the different types of machine learning and begin to think about the types of questions that can be reliably answered. There will never be any shortage of work for machine learning. The bottleneck is the number of people with the necessary expertise. As we’ve said before, another goal of healthcare.ai is to help commoditize machine learning in healthcare. If you’d like to get involved, please &lt;a href=&quot;https://github.com/HealthCatalystSLC/healthcareai-r&quot;&gt;start using and contributing to the package&lt;/a&gt; on your data and feel free to &lt;a href=&quot;http://healthcare.ai/contact&quot;&gt;reach out&lt;/a&gt; with questions!&lt;/p&gt;</content><author><name>Mike Mastanduno</name></author><category term="overview" /><summary type="html">Now that we have been through some of the applications of machine learning (ML) in mainstream technology, we thought it would be nice to give a broader overview of some of the different types of ML and how they might be applied to improve patient care. We explored the algorithms that currently make up healthcare.ai, and alluded to the fact that there is lots of room for expansion. We’ll take this post as an opportunity to speculate on where healthcare ML could go in the near and distant future. Along the way, we’ll discuss the different types of ML algorithms and give examples of their use in healthcare. 
At its most basic definition, machine learning refers to a group of algorithms that learn from data. These algorithms are different from conventional ones since they work using examples rather than rules. If you went to the hospital for flu like symptoms, a doctor thinking along the lines of traditional algorithms might say, “You have a fever, aches, general weakness, and no cold symptoms. This looks like the flu.” A different doctor, thinking like an ML algorithm, would say, “Hmmm, your symptoms are the same as 50 recent patients who had the flu. You probably do too.” Silly example, but it’s worth noting a couple of things. First, the ML doctor doesn’t actually need to know anything about the flu before they start making diagnoses. Second, their diagnoses probably won’t be very good until they have seen a lot of examples to compare against.</summary></entry><entry><title type="html">Which algorithms are in healthcare.ai?</title><link href="http://healthcare.ai/blog/2016/12/21/which-algorithms-are-in-healthcareai/" rel="alternate" type="text/html" title="Which algorithms are in healthcare.ai?" /><published>2016-12-21T20:17:11-07:00</published><updated>2016-12-21T20:17:11-07:00</updated><id>http://healthcare.ai/blog/2016/12/21/which-algorithms-are-in-healthcareai</id><content type="html" xml:base="http://healthcare.ai/blog/2016/12/21/which-algorithms-are-in-healthcareai/">&lt;p&gt;Machine learning has been around for decades and has been used to solve lots of problems. Some of these include &lt;a href=&quot;http://ats.cs.ut.ee/u/kt/hw/spam/spam.pdf&quot;&gt;spam filtering&lt;/a&gt; for email, &lt;a href=&quot;http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html&quot;&gt;suggestions on Netflix&lt;/a&gt;, &lt;a href=&quot;http://qz.com/571007/the-magic-that-makes-spotifys-discover-weekly-playlists-so-damn-good/&quot;&gt;optimized playlists on Spotify&lt;/a&gt;, &lt;a href=&quot;https://www.cs.umd.edu/~samir/498/Amazon-Recommendations.pdf&quot;&gt;custom recommendations on Amazon&lt;/a&gt;, &lt;a href=&quot;https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78#.7b9c9jmg7&quot;&gt;facial recognition on Facebook&lt;/a&gt;, &lt;a href=&quot;https://research.googleblog.com/2015/09/google-voice-search-faster-and-more.html&quot;&gt;voice recognition&lt;/a&gt; on your phone, &lt;a href=&quot;http://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html&quot;&gt;language translation&lt;/a&gt; on demand, &lt;a href=&quot;http://fusion.net/story/142326/the-new-google-photos-app-is-disturbingly-good-at-data-mining-your-photos/&quot;&gt;image search&lt;/a&gt; in your photo app, and &lt;a href=&quot;https://www.kaggle.com/wiki/DataScienceUseCases&quot;&gt;many more&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;While reading that long and varied list, you may be wondering where healthcare stands by comparison. Even though machine learning can solve many problems in healthcare, the field has not yet seen significant adoption. As &lt;a href=&quot;http://healthcare.ai/blog/2016/12/01/welcome-to-healthcareai/&quot;&gt;we’ve mentioned&lt;/a&gt;, the goal of healthcare.ai is to change that.&lt;/p&gt;

&lt;p&gt;We plan to bring the benefits of machine learning into healthcare by starting with the low-hanging fruit. Health Catalyst is a very practical company and that is reflected in &lt;a href=&quot;http://healthcare.ai/&quot;&gt;healthcare.ai&lt;/a&gt;. While many of the machine learning projects mentioned above are using advanced algorithms like &lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_learning&quot;&gt;deep learning&lt;/a&gt;, healthcare.ai is instead starting with the workhorses of the algorithm world. Note that this a different approach to healthcare machine learning compared to that of &lt;a href=&quot;https://research.google.com/teams/brain/healthcare/&quot;&gt;Google&lt;/a&gt; and &lt;a href=&quot;http://searchhealthit.techtarget.com/opinion/Microsoft-Project-Adam-may-reach-healthcare-specialties&quot;&gt;Microsoft&lt;/a&gt;, which are focusing on the sexier (but less practical) deep learning applications in healthcare.&lt;/p&gt;

&lt;p&gt;Before starting the discussion on healthcare.ai algorithm choices, I should note that we’ll focus on our R package and on &lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_classification&quot;&gt;classification&lt;/a&gt; (rather than regression), since &lt;em&gt;most&lt;/em&gt; problems in healthcare revolve around predicting &lt;a href=&quot;http://healthcare.ai/blog/2016/12/12/what-models-has-health-catalyst-created/&quot;&gt;Yes or No&lt;/a&gt; rather than a continuous variable. To make the terminology clear, it should also be stated that a machine learning algorithm, when paired with data, leads to a model. The algorithms exist off the shelf. The great value-add comes from pairing the proper algorithm with the data of interest. Healthcare.ai has open-sourced tools that allow you to easily match your data with suitable algorithms, create models, and help you answer your most important business questions. The models that you and Health Catalyst create are proprietary, but the tools used to make those models are free.&lt;/p&gt;

&lt;p&gt;Arguably the most simple and common algorithm when trying to classify things via machine learning is &lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_regression&quot;&gt;logistic regression&lt;/a&gt;. (Note that despite the name, this algorithm is used for classification problems.) We love it because it’s easy to use, quick to finish, and easy to interpret. We’ve been using this algorithm, but with a twist to it.&lt;/p&gt;

&lt;p&gt;We built healthcare.ai with the goal of providing users with guidance as to which &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_(machine_learning)&quot;&gt;features&lt;/a&gt; (i.e., variables) were predictive and worth using when building a model. This drove us to use &lt;a href=&quot;https://en.wikipedia.org/wiki/Lasso_(statistics)&quot;&gt;lasso&lt;/a&gt;, which is a linear model much like logistic regression, but it provides feedback on which features should or shouldn’t be included in the model. You might say, “Well, couldn’t logistic regression just ignore features that weren’t predictive?” Yes, but when the user has brought 20-40 variables into a focused dataset—what Health Catalyst calls a source area mart—to see if they help predict Sepsis, they’ll often only want to keep those variables that are predictive, as ETL processes often have hard resource constraints. With knowledge of how important each feature is, one can often remove many non-predictive variables from a model without any significant loss in accuracy.&lt;/p&gt;

&lt;p&gt;Next to linear models like lasso and logistic regression, the most common algorithm in machine learning is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_forest&quot;&gt;random forest&lt;/a&gt;. It’s different in that it can model non-linear relationships accurately. The random forest algorithm is an ensemble method, which aggregates the result of 100+ randomized decision trees to produce a prediction. While it’s a little harder to interpret than linear algorithms (like lasso), it typically doesn’t need much &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperparameter_optimization&quot;&gt;hyperparameter tuning&lt;/a&gt;, can run quite quickly, and from our experience often provides more accurate models (compared to linear algorithms) for common healthcare questions. These reasons drove us to include a random forest option in healthcare.ai.&lt;/p&gt;

&lt;p&gt;To back up a bit, in typical machine learning problems row order doesn’t matter. If you think of the simple example of housing data in Salt Lake City to determine the relationship between square footage and house price, it doesn’t matter if the row for house 10045 was listed before house 10057 in the dataset. Those two rows are treated independently. In longitudinal datasets however, the relationship between the rows often &lt;em&gt;does&lt;/em&gt; matter. In certain clinical datasets, we often find multiple entries for the same person, showing the person’s progression over time. When this progression is important to the business question that’s being addressed with machine learning, basic machine learning might not be suitable.&lt;/p&gt;

&lt;p&gt;This frequent longitudinal aspect of healthcare data is why we’re excited to offer linear &lt;a href=&quot;https://en.wikipedia.org/wiki/Mixed_model&quot;&gt;mixed models&lt;/a&gt; in our the R version of healthcare.ai. Mixed models offer the ability to combine a personal trend with a population trend. It’s called a mixed model because it combines fixed effects (i.e., those that relate to the population as a whole) with random effects (i.e., those that are innate to that individual). &lt;a href=&quot;http://www.bodowinter.com/tutorial/bw_LME_tutorial.pdf&quot;&gt;This paper&lt;/a&gt; provides a good introduction to the topic. From our experience, this algorithm is slow compared to random forest and lasso, but can create a better model if the prediction at hand significantly depends on a person’s history (i.e., think diabetic amputation risk rather than &lt;a href=&quot;https://en.wikipedia.org/wiki/Central_venous_catheter#Bloodstream_infections&quot;&gt;CLABSI&lt;/a&gt;). As always, healthcare.ai makes it easy to see how this algorithm performs on your dataset, and determine if it does a better job than the more common lasso and random forest algorithms.&lt;/p&gt;

&lt;p&gt;Again, the goal of healthcare.ai is to help the medical community use machine learning to improve healthcare outcomes. On the first pass, we have implemented what we feel are the simplest and most effective algorithms specific to healthcare data. We’ll certainly expand in the future, but for now there are a lot of efficiency gains to be &lt;achieved&gt;&lt;/achieved&gt; with basic algorithms, standardized performance metrics, smart implementation, and centralized documentation.&lt;/p&gt;

&lt;p&gt;If you want more detail, check out &lt;a href=&quot;https://github.com/HealthCatalystSLC/healthcareai-r&quot;&gt;our code&lt;/a&gt;. If you have questions or feedback, &lt;a href=&quot;http://healthcare.ai/contact&quot;&gt;contact us&lt;/a&gt;!&lt;/p&gt;</content><author><name>Levi Thatcher</name></author><category term="algorithms" /><summary type="html">Machine learning has been around for decades and has been used to solve lots of problems. Some of these include spam filtering for email, suggestions on Netflix, optimized playlists on Spotify, custom recommendations on Amazon, facial recognition on Facebook, voice recognition on your phone, language translation on demand, image search in your photo app, and many more.</summary></entry><entry><title type="html">Model evaluation using ROC Curves</title><link href="http://healthcare.ai/blog/2016/12/15/model-evaluation-using-roc-curves/" rel="alternate" type="text/html" title="Model evaluation using ROC Curves" /><published>2016-12-15T15:37:00-07:00</published><updated>2016-12-15T15:37:00-07:00</updated><id>http://healthcare.ai/blog/2016/12/15/model-evaluation-using-roc-curves</id><content type="html" xml:base="http://healthcare.ai/blog/2016/12/15/model-evaluation-using-roc-curves/">&lt;p&gt;Before a new technique in healthcare can be introduced to patient use, it must pass a rigorous set of quality standards. Then, to actually be adopted and see widespread use, a technique must be trusted and accepted by physicians and other front line care workers. For example, new drugs are evaluated in several steps before making into human trials, and then still have several hurdles to clear before they can be accepted as standard of care. Machine learning is poised to make a significant impact in clinical care in the near future, but it is not exempt from these same checks and developmental hurdles.&lt;/p&gt;

&lt;p&gt;The main goal of the healthcare.ai is to improve healthcare outcomes. As detailed in a previous post, we provide the tools and models that can use existing data to help intelligently guide clinical decisions. There has to be trust and transparency in these models if they are to make an impact and see long-term adoption. Just like a new drug, every model we build is evaluated to make sure that it’s high-quality before it is pushed into production. We evaluate models to compare them with other techniques, know when in their development they are ready for production, and to get an overall sense of how much we should trust them. Interestingly enough, one common way to do this (at least in classification problems) borrows from other areas of medicine and uses a &lt;em&gt;Receiver Operating Characteristic Curve (ROC).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Before we can get to the curve itself, we need a few definitions. Let’s say we’ve generated a machine learning model to predict the likelihood of 30-day readmission in a set of patients. The model gives a probability (between 0 and 1) for each person of how likely they are to be readmitted. 30 days later, the &lt;em&gt;True Positive Rate (TPR)&lt;/em&gt; is the proportion of actual readmissions that the test correctly predicted would be readmitted. The &lt;em&gt;False Positive Rate (FPR)&lt;/em&gt; is the proportion of patients whom the model predicted would be readmitted, but were not. In order to make these black and white predictions, we must pick a decision boundary somewhere between 0 and 1. Remember, the model gives a probability, not a definitive answer. If we were to choose 0.9, we would say that everyone with readmittance probability above 0.9 is a readmission, everyone below is not. We could then calculate the TPR and FPR, and have a measure of how well our model performed at 0.9 decision boundary. As you might have guessed, the decision boundary is a sticky spot. If we were choose 0.8 to increase the TPR, it will come at the expense of a larger FPR. The three parameters are tied to one another in a way that makes models hard to interpret and discuss.&lt;/p&gt;

&lt;p&gt;The ROC is a common way to avoid this. It is a graphical representation of the balance between TPR and FPR at &lt;em&gt;every&lt;/em&gt; possible decision boundary. The Area Under the Curve (AUC) is that magic solution that we have been looking for. The AUC is a single number that can evaluate a model’s performance, regardless of the chosen decision boundary. The perfect machine learning model will have an AUC of 1.0 (cyan), while a random one will have an AUC of 0.5 (orange). A good model will be over 0.7, a great one will be over 0.85. It might not be possible to perfectly classify a data set, but the AUC is a good way to compare models on that data, across patient cohorts, and give a sense of how trustworthy that model is in general.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/AUCPost_ROCExample.png&quot; alt=&quot;Example ROC Curves&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Whenever we are getting ready to deploy a model into use, we need to evaluate its overall performance. The AUC gives us a transparent, easy-to-interpret way to do that. Of course, it has limitations. For example, the usefulness of the ROC curve begins to break down with heavily imbalanced classes, obviously a big problem for healthcare data. One solution is to use AUC from a Precision-Recall Curve, but we’ll save that for a future post. If you’re interested in trying out ROC curves on your data, you’ll find some handy tools already built into the healthcare.ai package to help you evalutate your models. Finally, if you’re hungry for more, there are many &lt;a href=&quot;https://classeval.wordpress.com/introduction/introduction-to-the-roc-receiver-operating-characteristics-plot/&quot;&gt;great tutorials online&lt;/a&gt; for ROC curves.&lt;/p&gt;

&lt;p&gt;Thanks for reading and please &lt;a href=&quot;http://healthcare.ai/contact&quot;&gt;reach out&lt;/a&gt; with any questions or comments!&lt;/p&gt;</content><author><name>Mike Mastanduno</name></author><category term="overview" /><summary type="html">This blog will describe the motivation and uses of the ROC curve</summary></entry><entry><title type="html">What models has Health Catalyst created with healthcare.ai?</title><link href="http://healthcare.ai/blog/2016/12/12/what-models-has-health-catalyst-created/" rel="alternate" type="text/html" title="What models has Health Catalyst created with healthcare.ai?" /><published>2016-12-12T09:28:05-07:00</published><updated>2016-12-12T09:28:05-07:00</updated><id>http://healthcare.ai/blog/2016/12/12/what-models-has-health-catalyst-created</id><content type="html" xml:base="http://healthcare.ai/blog/2016/12/12/what-models-has-health-catalyst-created/">&lt;p&gt;After reading a few articles on healthcare.ai, some of you may be saying, well, that’s great–but what has Health Catalyst actually used it for? Since Health Catalyst has been open with sharing the tool set, it only makes sense that they’d also be willing to share details of its use. As the Director of Data Science at Health Catalyst and founder of healthcare.ai, I oversee all client predictive engagements, and will make a point of frequently updating the community on our work. If you have questions, comments, or criticism, please &lt;a href=&quot;http://healthcare.ai/contact&quot;&gt;reach out&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The goal of healthcare.ai was to provide a simple, flexible tool to streamline healthcare machine learning. This means that it works across financial, operational, and clinical realms. If a health system has a business question that they want predictions for, we will make healthcare.ai flexible enough to cover that use case. Today we’ll briefly cover three recent predictive project, and detail more for a future post.&lt;/p&gt;

&lt;p&gt;Let’s start with finance. Uncompensated care is a growing problem at most health systems. To help a counter this trend, we’ve started creating propensity-to-pay models. Recall that each health system interested in using machine learning is provided a custom model, tailored to their data. In this propensity-to-pay project, for each person with an open account with the health system, each month the probability of payment is calculated. This personal probability can be used to determine 1) who may need reminders, 2) who may need financial assistance, and 3) how the likelihood of payment changes over time and after particular life events. A few &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_(machine_learning)&quot;&gt;features&lt;/a&gt; that were important to this model turned were things like whether the person paid last month (surprise!), account balance, a person’s age, the month of the year, etc. These may vary for your health system (as your demographics are likely a bit different), but healthcare.ai makes it easy to customize the model to &lt;em&gt;your&lt;/em&gt; payment data.&lt;/p&gt;

&lt;p&gt;Those who have ever worked in a clinical setting know how hard it is to maintain schedules that keep both clinicians and patients happy. Slots are often over or under-booked because someone showed up late, didn’t show up, or showed up without warning. Health Catalyst taken a first pass at this problem via a no-show predictive model. In this engagement, we gathered all past data on the characteristics of people that had showed or hadn’t showed for their appointments, and created an accurate predictive model to assess, with each scheduled appointment, the risk of a no-show. If the clinic feels that they can reduce their no-show rate by extra phone calls (or other measures), this predictive guidnaces assures that the resources used are efficiently allocated. If, on the other hand, the clinic has found that it’s quite hard to reduce the no-show rate (even with this guidance), they can use the probability scores to over-book particular slots, such that clinical scheduling is optimized using past data. &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_(machine_learning)&quot;&gt;Features&lt;/a&gt; that were particularly helpful in this prediction were prior number of cancellations, appointment type, and week of the year.&lt;/p&gt;

&lt;p&gt;Finally, we’ll touch on a clinical case. Many health systems are &lt;a href=&quot;http://www.modernhealthcare.com/article/20150803/NEWS/150809981&quot;&gt;penalized&lt;/a&gt; if their 30-day readmissions rate is too high. While general readmissions models are possible via healthcare.ai, we’ve found that focusing on particular disease cohorts (such as for heart failure, sepsis, or COPD) allows us to create a much more accurate model. We’ve had multiple engagements recently related to 30 and 90-day COPD readmissions. How it works is that the relevant data on past patients (and whether they had a readmission or not) is collected into a &lt;a href=&quot;https://www.healthcatalyst.com/late-binding-data-warehouse/late-binding-data-bus/sam-designer/&quot;&gt;subject area mart&lt;/a&gt;, and a couple of different algorithms are used to create models. Once we find the column set and algorithm that together produce the most accurate model, we save the model and integrate it into our nightly &lt;a href=&quot;https://en.wikipedia.org/wiki/Extract,_transform,_load&quot;&gt;ETL&lt;/a&gt;. This way, clinicians  receive daily guidance as to which of their patients is most likely to be readmitted. Among others, &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_(machine_learning)&quot;&gt;features&lt;/a&gt; like prior readmissions, pre-existing conditions, and specific facility were particularly helpful to this model.&lt;/p&gt;

&lt;p&gt;Considering the resource constraints present in many hospital units, this type of machine learning guidance can be crucial to efficiently deploying resources toward achieving business goals (i.e., reducing readmissions, reducing 1-yr mortality, preventing &lt;a href=&quot;https://www.cdc.gov/hai/&quot;&gt;HAIs&lt;/a&gt;, etc). As time goes on, we’ll detail more of these predictive projects, and explain how they might be useful to your health system.&lt;/p&gt;

&lt;p&gt;Thanks, and please &lt;a href=&quot;http://healthcare.ai/contact.html&quot;&gt;reach out&lt;/a&gt; with any questions!&lt;/p&gt;</content><author><name>Levi Thatcher</name></author><category term="overview" /><summary type="html">After reading a few articles on healthcare.ai, some of you may be saying, well, that’s great–but what has Health Catalyst actually used it for? Since Health Catalyst has been open with sharing the tool set, it only makes sense that they’d also be willing to share details of its use. As the Director of Data Science at Health Catalyst and founder of healthcare.ai, I oversee all client predictive engagements, and will make a point of frequently updating the community on our work. If you have questions, comments, or criticism, please reach out.</summary></entry></feed>
