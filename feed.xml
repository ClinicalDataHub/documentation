<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xml" href="/feed.xslt.xml"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.3.1">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2017-02-10T12:47:21-07:00</updated><id>//</id><title type="html">healthcare.ai</title><entry><title type="html">How can feature importance guidance help with model creation?</title><link href="/blog/2017/02/10/Gaugingfeatureimportance/" rel="alternate" type="text/html" title="How can feature importance guidance help with model creation?" /><published>2017-02-10T12:00:00-07:00</published><updated>2017-02-10T12:00:00-07:00</updated><id>/blog/2017/02/10/Gaugingfeatureimportance</id><content type="html" xml:base="/blog/2017/02/10/Gaugingfeatureimportance/">&lt;p&gt;In designing healthcare.ai, we are excited to provide practical machine learning (ML) guidance to health data folks, and feature importance guidance is near the top of the practicality list. When combining algorithms and data to create models, often you don’t want to keep all of your initial features (i.e., input columns) in your &lt;em&gt;final&lt;/em&gt; model. You might be wondering why–wouldn’t the model be smart enough to adjust for non-helpful features, such that each feature is used appropriately? Certainly! But, it’s often the case that production environments are resource-constrained by frequent data loads and transformations.&lt;/p&gt;

&lt;p&gt;Typically, when working in healthcare, there will be certain data &lt;em&gt;sources&lt;/em&gt;–these can be EMRs, payer data, census data, etc. Before using ML to create a model, you’ll often create a data mart that will only contain certain columns (that are related) from one or more of those primary databases (i.e., data sources). Health Catalyst calls these source area marts (SAMs). The columns in these SAMs, which can come from different &lt;em&gt;sources&lt;/em&gt;, are combined via a key column, which could be a unique patient identifier. This way we can get a coherent picture of health system’s sepsis patients, for example, using data from several sources. To get to the point, all of these combinations and transformations take significant resources each night, and thus you’ll often want to remove unhelpful columns from your SAM. This is where feature importance comes in.&lt;/p&gt;

&lt;p&gt;Recall that healthare.ai offers two main practical algorithms. &lt;a href=&quot;https://en.wikipedia.org/wiki/Lasso_(statistics)&quot;&gt;Lasso&lt;/a&gt; is our linear offering, and we chose it over linear regression because the algorithm provides guidance on which features can be removed from a model, without sacrificing much performance. Lasso is actually a recent development, with it being published &lt;a href=&quot;http://www.jstor.org/stable/2346178?seq=1#fndtn-page_scan_tab_contents&quot;&gt;first in 1996&lt;/a&gt;. How does it help with feature selection? Professor Tibshirani explains it &lt;a href=&quot;http://www.jstor.org/stable/2346178?seq=1#fndtn-page_scan_tab_contents&quot;&gt;by saying&lt;/a&gt; “the ‘lasso’ minimizes the residual sum of squares &lt;em&gt;subject to the sum of the absolute value of the coefficients&lt;/em&gt; being less than a constant.” In simpler terms, the key difference from a plain linear model is that lasso, when converging, takes into account the size of the coefficient weights and drives the smallest of them to zero.&lt;/p&gt;

&lt;p&gt;This helps in multiple ways. First, one receives guidance on which features can be removed from the SAM before deploying a final model since they were not used in prediction. The second (related) benefit is that the model is more resistant to overfitting. Finally, the model is more interpretable since a user knows exactly which features were used.&lt;/p&gt;

&lt;p&gt;The other pracital algorithm offered by healthcare.ai is the random forest. Whereas lasso is a general linear algorithm, the &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_forest#cite_note-ho1998-2&quot;&gt;random forest&lt;/a&gt; is based on an ensemble of decision trees. Not only is it a workhorse of the machine learning world (due to its flexibility and ease of use), but it’s also relatively new. See &lt;a href=&quot;http://ect.bell-labs.com/who/tkh/publications/papers/odt.pdf&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://www.stat.berkeley.edu/~breiman/randomforests-rev.pdf&quot;&gt;here&lt;/a&gt; for the origin story–note that &lt;a href=&quot;https://en.wikipedia.org/wiki/Leo_Breiman&quot;&gt;Leo Breiman&lt;/a&gt; was involved in the early stages of both lasso and random forests. We chose the random forest for healthcare.ai not only because of its flexibility on various kinds of data, but also because it offers up feature selection guidance. It does this via &lt;a href=&quot;http://stats.stackexchange.com/a/92843/124897&quot;&gt;Gini importance&lt;/a&gt;, which takes into account the number of times a particular feature is higher up in each of the 100-200 trees in the random forest. Recall that the feature chosen at the top of the decision tree is the one with the highest impact on the dependent (i.e., output) variable. Note that this guidance &lt;a href=&quot;http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-25&quot;&gt;isn’t perfect&lt;/a&gt;–it tends to prefer continuous features and those with many categories–but overall, it helps healthcare.ai offer the ability to produce both accurate models &lt;em&gt;and&lt;/em&gt; unbloated datasets.&lt;/p&gt;

&lt;p&gt;We’re excited to learn from you how these tools are helping on &lt;em&gt;your&lt;/em&gt; health data problems and where they can be improved, so please &lt;a href=&quot;/contact.html&quot;&gt;reach out&lt;/a&gt;.&lt;/p&gt;</content><author><name>Levi Thatcher</name></author><category term="algorithms" /><summary type="html">Feature importance guidance offers several benefits to those doing machine learning.</summary></entry><entry><title type="html">Using healthcare.ai Python</title><link href="/blog/2017/02/03/pathology-wisc-cancer/" rel="alternate" type="text/html" title="Using healthcare.ai Python" /><published>2017-02-03T06:00:00-07:00</published><updated>2017-02-03T06:00:00-07:00</updated><id>/blog/2017/02/03/pathology-wisc-cancer</id><content type="html" xml:base="/blog/2017/02/03/pathology-wisc-cancer/">&lt;p&gt;Note: this post follows a &lt;a href=&quot;/notebooks/Wisconsin-Pathology-Notebook.html&quot;&gt;Jupyter Notebook&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We have had &lt;a href=&quot;/blog/2017/01/17/using-r-for-data-analysis/&quot;&gt;a lot of say&lt;/a&gt; about R on the blog lately! R is a great statistical language to do data manipulation, cleaning, machine learning, and visualization. One reason we really like it on our team is how neatly packaged the tools are. However, there is a significant portion of the machine learning community that uses Python. For our purposes, the benefits of python mainly relate to speed, deep learning, and the ease of working with massive datasets. Until one language is markedly more common than the other, we maintain a &lt;a href=&quot;hcai-py&quot;&gt;healthcare.ai&lt;/a&gt; Python package in addition to the R package. This post serves to give an example of how to use the python package.&lt;/p&gt;

&lt;p&gt;This example uses a dataset that can be found on the &lt;a href=&quot;https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29&quot;&gt;UCI Machine Learning Repository&lt;/a&gt; and is freely available for download. The data is an array of characteristics from pathology samples. The process to get the data worked like this:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;A patient with suspected breast cancer had a sample of cells taken from the lesion in a biopsy procedure.&lt;/li&gt;
  &lt;li&gt;Cells from the sample were put on a pathology slide and stained to highlight biological characteristics.&lt;/li&gt;
  &lt;li&gt;A pathologist read the slides to determine whether the cells were malignant or benign.&lt;/li&gt;
  &lt;li&gt;The pathologist recorded observations about the size, shape, and characteristics of the cell nuclei.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/breast-cancer-images-small.png&quot; alt=&quot;Pathology slide example&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The cellular characteristics in the slides help the pathologist make the distinction between malignant and benign lesions. Since the pathologist had the courtesy to record the features, we can see if a machine learning algorithm can distinguish the tissue samples as well as the pathologist could. Like &lt;a href=&quot;/blog/2017/01/08/us-health-by-county/&quot;&gt;R notebooks&lt;/a&gt;, Python can be written into a &lt;em&gt;Jupyter Notebook&lt;/em&gt; for easy annotation and sharing. As there is a lot of code, data, and visualization contained within this post, it would be good if you would follow along with the &lt;a href=&quot;nb&quot;&gt;notebook&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The basic process is:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Load the data and healthcare.ai&lt;/li&gt;
  &lt;li&gt;Explore the data through statistics and visualization.&lt;/li&gt;
  &lt;li&gt;Implement machine learning models.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Again, check out the &lt;a href=&quot;/notebooks/Wisconsin-Pathology-Notebook.html&quot;&gt;example notebook&lt;/a&gt;, thanks for reading, and &lt;a href=&quot;/contact.html&quot;&gt;contact us&lt;/a&gt; with any questions.&lt;/p&gt;</content><author><name>Mike Mastanduno</name></author><category term="example" /><summary type="html">This is an example of how to build a predictive model using Python and healthcare.ai</summary></entry><entry><title type="html">How can precision-recall curves help evaluate your model?</title><link href="/blog/2017/01/31/au-pr-vs-au-roc/" rel="alternate" type="text/html" title="How can precision-recall curves help evaluate your model?" /><published>2017-01-31T03:00:00-07:00</published><updated>2017-01-31T03:00:00-07:00</updated><id>/blog/2017/01/31/au-pr-vs-au-roc</id><content type="html" xml:base="/blog/2017/01/31/au-pr-vs-au-roc/">&lt;p&gt;Whenever we use machine learning (ML) for prediction, the question of how to evaluate the model is of the utmost importance. Of course, different metrics are appropriate for different kinds of models and business questions. If we were to predict patient length of stay (LOS)—a numeric value—we’d use a different model evaluation metric than if we were evaluating a model that predicts 30-day readmissions, which is a binary column (i.e., Y or N). It’s these types of decisions that &lt;a href=&quot;http://healthcare.ai/&quot;&gt;healthcare.ai&lt;/a&gt; streamlines.&lt;/p&gt;

&lt;p&gt;In healthcare, we’re most commonly focused binary predictions—think of readmissions, hospital-acquired infections, mortality, appointment no-shows, etc. In addition, our data frequently has &lt;em&gt;imbalanced classes&lt;/em&gt;. What this means is that, for the column we’re predicting (i.e., no-shows, readmissions, etc.), there are many more of either Y’s or N’s. In healthcare ML, it’s typically the case that N is much more common than Y. That’s a good thing. It’s nice that most people don’t get a hospital-acquired infection. However, because of this label imbalance, the most common classification evaluation metric could benefit from some adjustments.&lt;/p&gt;

&lt;p&gt;For binary classification, the gold-standard for evaluation is the ROC curve and the associated area under the curve (AUC). &lt;a href=&quot;http://healthcare.ai/blog/2016/12/15/model-evaluation-using-roc-curves/&quot;&gt;In a past post&lt;/a&gt;, we’ve detailed why these are common. Considering that imbalanced classes are so pervasive in healthcare, we’re excited to detail why we’ve also included precision-recall (PR) curves and the associated area under the curve in the R 0.1.10 release of healthcare.ai. In the following discussion, we’ll focus on the curves themselves, but the same thoughts apply to both AU_ROC and AU_PR, which is area under the ROC and PR curves, respectively.&lt;/p&gt;

&lt;p&gt;Recall that the &lt;a href=&quot;http://healthcare.ai/blog/2016/12/15/model-evaluation-using-roc-curves/&quot;&gt;ROC&lt;/a&gt; shows the &lt;em&gt;True Positive Rate (TPR)&lt;/em&gt; on the y-axis and the &lt;em&gt;False Positive Rate (FPR)&lt;/em&gt; on the X-axis. As that &lt;a href=&quot;http://healthcare.ai/blog/2016/12/15/model-evaluation-using-roc-curves/&quot;&gt;previous post&lt;/a&gt; explained, &lt;em&gt;TPR&lt;/em&gt; is the proportion of actual readmissions that the test correctly predicted would be readmitted. &lt;em&gt;FPR&lt;/em&gt; is the proportion of patients whom the model predicted would be readmitted, but were not. Here’s a ROC curve, for reference.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/AUCPost_ROCExample.png&quot; alt=&quot;Example ROC Curves&quot; /&gt;&lt;/p&gt;

&lt;p&gt;TPR is also known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall#Definition_.28classification_context.29&quot;&gt;&lt;em&gt;recall&lt;/em&gt;&lt;/a&gt; and also is present in PR curves. Compared to a ROC curve, a PR curve simple moves TPR (i.e., recall) to the x-axis and swaps out FPR for precision (which is on the y-axis). Now, why is that so significant when it comes to data with imbalanced classes? Consider the true negative. When you have data with a 10% rate of 30-day readmissions and that’s what you’re predicting, your model could simply predict N 90% of the time and be correct. Introducing precision instead of FPR in the PR curve is helpful because, as &lt;a href=&quot;http://machinelearning.wustl.edu/mlpapers/paper_files/icml2006_DavisG06.pdf&quot;&gt;this paper&lt;/a&gt; states, “a large change in the number of false positives can lead to a small change in the false positive rate.” Basically, with highly-imbalanced classes FPR is overwhelmed by the number of true negatives. Precision, on the other hand, focuses on true positives and false positives (which are much more relevant than true negatives when your classes are imbalanced toward negatives). We’ve created a guide that was inspired by this Stack Overflow &lt;a href=&quot;http://stats.stackexchange.com/a/90783/124897&quot;&gt;answer&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/Post17PRcurve/Equations_final.jpg&quot; alt=&quot;Equations&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note the lack of TNs in the second set of equations. Now, &lt;em&gt;how do the two metrics compare on the same dataset?&lt;/em&gt; Let’s use the &lt;a href=&quot;https://github.com/HealthCatalystSLC/healthcareai-r/blob/master/inst/extdata/HCRDiabetesClinical.csv&quot;&gt;diabetes dataset&lt;/a&gt; that comes with healthcare.ai to compare Lasso vs. Random Forest when predicting readmissions:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/Post17PRcurve/ROCExample.png&quot; alt=&quot;ROCExample&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/Post17PRcurve/PRCurveExample.png&quot; alt=&quot;PRExample&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that the Random Forest-based model performs better than the Lasso-based model—note the greater area under the curve. However, we find that the difference between the algorithms is much more stark in terms of the ROC curve compared with the PR curve. Is this typically the case?&lt;/p&gt;

&lt;p&gt;As we read in &lt;a href=&quot;http://machinelearning.wustl.edu/mlpapers/paper_files/icml2006_DavisG06.pdf&quot;&gt;this paper&lt;/a&gt;, “an algorithm that optimizes the area under the ROC curve is not guaranteed to optimize the area under the PR curve.” If you have highly imbalanced classes, you may want to focus on the PR curve and maximizing the area under the curve (i.e., AU_PR), even though common benchmarks and your related presentation may tout your awesome results in terms of ROC. Happy modeling!&lt;/p&gt;

&lt;p&gt;Feel free to &lt;a href=&quot;http://healthcare.ai/contact.html&quot;&gt;reach out with questions&lt;/a&gt;, and thanks for reading.&lt;/p&gt;</content><author><name>Levi Thatcher</name></author><category term="overview" /><summary type="html">While area under the ROC is a common metric for binary classification, area under the PR curve is often more appropriate.</summary></entry><entry><title type="html">How should you divide your data?</title><link href="/blog/2017/01/26/training-testing/" rel="alternate" type="text/html" title="How should you divide your data?" /><published>2017-01-26T03:00:00-07:00</published><updated>2017-01-26T03:00:00-07:00</updated><id>/blog/2017/01/26/training-testing</id><content type="html" xml:base="/blog/2017/01/26/training-testing/">&lt;p&gt;At the most basic level, Machine Learning (ML) is a category of algorithms that learn from historical data and generalize to future data. Having good data is becoming more and more important to successful organizations today. It’s almost &lt;a href=&quot;http://www.nytimes.com/2012/02/12/sunday-review/big-datas-impact-in-the-world.html&quot;&gt;becoming a form of currency&lt;/a&gt;. But having access to big data is only the first step. Using it effectively is another matter entirely. Healthcare organizations have collected data for years, but data will always be a finite resource. This post seeks to help you decide how to best allocate your data resource for ML training, testing, and “in-the-wild” evaluation.&lt;/p&gt;

&lt;p&gt;When developing or training an ML model, you are trying to feed an algorithm as much data as possible to learn from. This is called the &lt;em&gt;training set&lt;/em&gt;. The model learns the patterns within that dataset and uses them to predict the outcome variable. When working on the training set, you will &lt;a href=&quot;http://healthcare.ai/blog/2017/01/24/feature-engineering/&quot;&gt;engineer features&lt;/a&gt;, &lt;a href=&quot;https://www.quora.com/What-are-hyperparameters-in-machine-learning&quot;&gt;tune hyperparameters&lt;/a&gt;, and select the &lt;a href=&quot;http://healthcare.ai/blog/2016/12/21/which-algorithms-are-in-healthcareai/&quot;&gt;ML algorithm&lt;/a&gt;. It might be possible to get your model to be perfectly &lt;a href=&quot;http://healthcare.ai/blog/2016/12/15/model-evaluation-using-roc-curves/&quot;&gt;accurate&lt;/a&gt; on the training set. But remember, that is not the ultimate goal! You want your model to be perfect on &lt;strong&gt;all&lt;/strong&gt; data. In other words, you want it to generalize well to new data.&lt;/p&gt;

&lt;p&gt;To generalize to new data, you must reserve some of your data for validation, or testing. A typical distribution is to use 60-80% of the data for training and the remaining for testing. If the model performs comparably well on the testing data, you can be fairly confident that the model will generalize well to new data. When splitting data into training and testing sets, it is ideal to have similarly distributed datasets in both groups. To do so, there are several things to keep in mind:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Randomize row numbers for training and testing. This avoids complications resulting from using separate blocks of time for training vs. testing. As an example, flu symptoms would be more common in the winter and you’d want those to appear at the same rate in both sets.&lt;/li&gt;
  &lt;li&gt;Keep positive and negative examples balanced. If you have a data set with 10% positive examples, try to divide those evenly. You don’t want your model to learn from only one type of example. For example, it is much more common for a patient to NOT be readmitted. &lt;a href=&quot;http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/&quot;&gt;Imbalanced classes&lt;/a&gt; are a common difficulty that we’ll cover more thoroughly in a future post.&lt;/li&gt;
  &lt;li&gt;Think about the distributions within individual columns and whether random sampling is likely to result in similarly distributed training and test sets. If it won’t, you might need to manually sample.
In practice, a good first step is to randomly assign groups and &lt;a href=&quot;http://healthcare.ai/blog/2016/12/15/model-evaluation-using-roc-curves/&quot;&gt;evaluate your model’s performance&lt;/a&gt; on 80% training and 20% testing sets. If the model is highly accurate across both data sets, it is ready to move on to the &lt;em&gt;deployment&lt;/em&gt; step.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the deployment step, the model is put into production and left to make new predictions on new data examples. We sometimes refer to this period as “in-the-wild” testing, where the model is deployed on the servers and is interacting with real data. We aren’t using the model to inform decisions yet, but it’s in the last stage of evaluations. It’s worth noting that we are not changing the model here, but using the saved model from development. In this phase, we are looking to see that the performance is the same in the wild as it was in development training and testing. If it’s not, the model must be debugged for issues like &lt;a href=&quot;http://healthcare.ai/blog/2017/01/06/data-leakage-in-healthcare-machine-learning/&quot;&gt;data leakage&lt;/a&gt; or &lt;a href=&quot;http://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/&quot;&gt;overfitting&lt;/a&gt;. The length of time required for this step depends on the target variable. For example, a 30-day readmission model will require at least 30 days of in-the-wild testing. If testing in the wild is successful, the model is ready to move into production and help guide decisions.&lt;/p&gt;

&lt;p&gt;Building an ML model is often a difficult process. A large dataset is the first step, but it must be cleaned and prepared. The model must be trained while keeping many small things in mind. It can be a tiresome process of guess and check, but experience with ML and domain knowledge of the data can speed things up. Additionally, many of these steps, such as randomizing data, can be automated. One focus of &lt;a href=&quot;http://healthcare.ai/&quot;&gt;healthcare.ai&lt;/a&gt; is helping to reduce the number of things a user has to keep track of while model building.&lt;/p&gt;

&lt;p&gt;Hopefully this post helps you get the most out of your dataset, and you can use it to build models that generalize well to new data. As always, feel free to &lt;a href=&quot;http://healthcare.ai/contact.html&quot;&gt;reach out with questions&lt;/a&gt;, and thanks for reading.&lt;/p&gt;</content><author><name>Mike Mastanduno</name></author><category term="overview" /><summary type="html">This is an overview of training and testing data sets. And of ML models in development and deployment.</summary></entry><entry><title type="html">Feature engineering in healthcare machine learning</title><link href="/blog/2017/01/24/feature-engineering/" rel="alternate" type="text/html" title="Feature engineering in healthcare machine learning" /><published>2017-01-24T03:00:00-07:00</published><updated>2017-01-24T03:00:00-07:00</updated><id>/blog/2017/01/24/feature-engineering</id><content type="html" xml:base="/blog/2017/01/24/feature-engineering/">&lt;p&gt;In previous blog posts, we’ve discussed specific &lt;a href=&quot;http://healthcare.ai/blog/2016/12/22/applications-of-healthcare-machine-learning/&quot;&gt;applications of machine learning (ML) in healthcare&lt;/a&gt; and the &lt;a href=&quot;http://healthcare.ai/blog/2016/12/21/which-algorithms-are-in-healthcareai/&quot;&gt;available algorithms in healthcare.ai&lt;/a&gt;. As you build an ML model, creating and selecting the right features can be just as foundationally important as matching the right algorithm with the right use case. In this post, we will discuss how domain knowledge of healthcare data can be used to create features that make your ML models more accurate and useful. This process is known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_engineering&quot;&gt;feature engineering&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Using an ML model off the shelf is easy but often ineffective. Feature engineering usually makes or breaks it. It’s the creative part of the process that builds the foundation for everything else. Many of the features we use in our ML models can be categorized as demographics, vitals, labs, medications, clinical events, healthcare utilization, visit details, and comorbidities. We can break these categories out more granularly into the specific features that are helpful to our ML model. From here, we often develop multiple transformations of a feature based on the underlying reasons that might inherently make that specific form of the feature more predictive—this is the engineering part.&lt;/p&gt;

&lt;p&gt;For example, a patient’s blood pressure could be a clinical event feature that is predictive of their hospital length of stay. However, there are a bunch of things you need to consider before arriving at a truly predictive feature: Should you separate systolic from diastolic? Create a binary feature, hypotension, or hypertension? Or continuous? When multiple records exist, do you use the highest values or the lowest? Earliest or latest? And so on. You might even need to get so specific as to create a binary (Yes/No) feature defined by the question, “Was the patient’s systolic blood pressure 140 mmHg or higher during the first 12 hours following admission to the hospital?” or a continuous feature defined as the “Highest diastolic blood pressure value within the first 24 hours of admission to the hospital.”&lt;/p&gt;

&lt;p&gt;For clarity’s sake, two additional examples of basic feature engineering that we use regularly are age and home address. A feature containing continuous age values could be helpful to the model on its own, or transformed into 10-year age groups or a binary variable identifying patients age 65 and older. An address can be broken out into its various components like city, state, and zip code, all of which can serve as proxies for helpful information that is not always available in the data, like socioeconomic status. We can also use &lt;a href=&quot;https://en.wikipedia.org/wiki/Geocoding&quot;&gt;geocoding&lt;/a&gt; to convert addresses into coordinates that can be plotted against relevant maps to pick up additional insight related to things like distance to a provider or local barriers to transportation.&lt;/p&gt;

&lt;p&gt;Clearly, you could come up with dozens of versions of a single feature, and some are going to work much better than others. The great thing about ML is that you can use your healthcare domain knowledge to pare down the relevant and potentially useful iterations of your feature, and then let the ML algorithm help isolate the most predictive features and transformations during final &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_selection&quot;&gt;feature selection&lt;/a&gt; (a related topic that we’ll dive into in an upcoming post). The ML algorithms in &lt;a href=&quot;http://healthcare.ai/&quot;&gt;healthcare.ai&lt;/a&gt; can help select the right versions, but you have to do the leg work to get them in there to try. Also, we covered this recently, but don’t forget about &lt;a href=&quot;http://healthcare.ai/blog/2017/01/06/data-leakage-in-healthcare-machine-learning/&quot;&gt;data leakage&lt;/a&gt; when creating features; you’ll need to make sure you consider your use case so that you don’t train the ML model using features that would not yet be available at your preferred time of prediction.&lt;/p&gt;

&lt;p&gt;Feature engineering is a ton of work and underscores the importance of data scientists and data architects having a combination of technical skills and deep domain knowledge. Some good news related to lightening the workload—we’ve already included a couple of feature engineering functions in healthcare.ai that can save a significant amount of time and effort transforming certain features.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://healthcare.ai/r/model-pre-processing/longitudinal-imputation/&quot;&gt;Longitudinal imputation&lt;/a&gt; works with longitudinal data to carry a value forward from a patient’s previous record to the fill in a NULL value in the most current record. An example might be using longitudinal imputation to pull a patient’s BMI forward from their previous visit to their current visit if their weight and height have not yet been recorded. Using this function appropriately can improve model accuracy by filling in data that might otherwise be left NULL or imputed with your dataset’s mean or mode for that feature.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://healthcare.ai/r/model-pre-processing/seasonality-handling/&quot;&gt;Seasonality handling&lt;/a&gt; transforms date-time columns, which are difficult for ML algorithms to handle, into their various components, ultimately treating them as separate features. Event dates, like date of admission, can hold a lot of helpful information for seasonality handling and proxying for other things that might not be well represented in the data. With minimal effort, you can use this function to test which component(s) of the admit date-time stamp like month, day of week, hour, etc., are most helpful to your model.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Automating feature engineering is difficult to do, especially considering the complexity of healthcare data and the types of healthcare ML problems we are attempting to solve. Deep domain knowledge and human intuition are very difficult to replicate. However, it’s a hot area of research and we’ll definitely continue to explore and post on advances in feature engineering. Algorithms for automatically generating features for relational data sets like the &lt;a href=&quot;http://groups.csail.mit.edu/EVO-DesignOpt/groupWebSite/uploads/Site/DSAA_DSM_2015.pdf&quot;&gt;Deep Feature Synthesis algorithm&lt;/a&gt; are becoming smarter and more dynamic every day. We look forward to incorporating new techniques and algorithms into healthcare.ai so that we can all focus our efforts on improving healthcare outcomes in new and exciting ways.&lt;/p&gt;

&lt;p&gt;Thanks for reading, and please &lt;a href=&quot;http://healthcare.ai/contact&quot;&gt;reach out&lt;/a&gt; with any questions or comments!&lt;/p&gt;</content><author><name>Taylor Larsen</name></author><category term="overview" /><summary type="html">This blog will discuss how domain knowledge of healthcare data can be used to create features that make ML models more accurate and useful</summary></entry><entry><title type="html">Survey of deep learning in radiology</title><link href="/blog/2017/01/19/survey-of-deep-learning-in-radiology/" rel="alternate" type="text/html" title="Survey of deep learning in radiology" /><published>2017-01-19T16:32:11-07:00</published><updated>2017-01-19T16:32:11-07:00</updated><id>/blog/2017/01/19/survey-of-deep-learning-in-radiology</id><content type="html" xml:base="/blog/2017/01/19/survey-of-deep-learning-in-radiology/">&lt;p&gt;This blog has been talking a lot about Machine Learning (ML) with regard to tabular data. That makes sense because predictive algorithms based on tabular data are often easy to implement and have a lot of potential to improve outcomes. Also, we have access to a lot of tabular data from the EHR.  However, ML is capable of doing a lot more than predicting probabilities on tabular data, and there are incredible opportunities in other areas of healthcare. One in particular is in Radiology and Pathology departments. These departments generate tabular data, but their bread and butter is image data. Beth Israel Deaconess Medical Center (Harvard), for instance, generates approximately 20 terabytes of image data per year (vs. 1 TB text data) &lt;a href=&quot;http://geekdoctor.blogspot.com/2011/04/cost-of-storing-patient-records.html&quot;&gt;source&lt;/a&gt;. That’s a lot of data to potentially use!&lt;/p&gt;

&lt;p&gt;With all the talk around deep learning lately, it would be hard for an ML group to avoid at least discussing whether or not it would be practical to develop our own applications. While deep learning is absolutely a buzz word, it truely does present a number of possibilities for future innovations. In only a handful of years, deep learning will &lt;a href=&quot;https://www.tesla.com/autopilot&quot;&gt;driving our cars&lt;/a&gt;, doing &lt;a href=&quot;http://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html?smid=pl-share&amp;amp;_r=0&quot;&gt;real-time translation of spoken language&lt;/a&gt;, allowing you to &lt;a href=&quot;https://www.ditto.com/&quot;&gt;virtually try on glasses online&lt;/a&gt;, and many other amazing things that we haven’t even imagined yet. With this kind of limitless potential, it isn’t hard to imagine an opportunity for computers to read medical images, as outlined in this paper from the &lt;a href=&quot;http://www.nejm.org/doi/full/10.1056/NEJMp1606181&quot;&gt;New England Journal of Medicine&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;One of the great things about working on &lt;a href=&quot;http://healthcare.ai&quot;&gt;healthcare.ai&lt;/a&gt; is the opportunity to plan and shape &lt;a href=&quot;http://healthcare.ai/blog/2016/12/21/which-algorithms-are-in-healthcareai/&quot;&gt;what the package is capable of&lt;/a&gt;. There are things that we are working towards in the near future, like unsupervised learning, and there is functionality that is still not mainstream and will require a more concentrated research effort for us to be able to implement. &lt;strong&gt;The purpose of this post is to try to provide a survey of machine learning on medical imaging, and where the lowest hanging fruit might be.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;One area, &lt;a href=&quot;https://www.cancer.gov/types/breast/mammograms-fact-sheet#q1&quot;&gt;mammographic screening&lt;/a&gt;, is a particularly promising application of deep learning. Current guidelines recommend annual or biennial breast cancer screening with mammography for women over 40 or 50 (depending on the guideline). This means we do a lot of expensive screening, and it’s not always effective. Deep learning could help improve the effectiveness of that screening to improve outcomes and reduce costs. Take a look at &lt;a href=&quot;http://ashevillegynecologywellness.com/wp-content/uploads/2016/05/MammoGram-Seriesshow.jpg&quot;&gt;these images&lt;/a&gt;. Even if you are not a trained radiologist, you are probably able to make medically relevant observations about them. The normal mammogram looks rather indistinct. There are brighter areas and darker areas, but they are mostly &lt;a href=&quot;http://www.facingourrisk.org/our-role-and-impact/advocacy/documents/breast-screening-comparison-chart.pdf&quot;&gt;consistent&lt;/a&gt;. The second image from the left has a large, round, and uniformly bright spot, almost a sure sign of a cyst. Finally, the cancer image has a very bright spot that seems to have tentacles extending from a central structure. While real images can be extremely subtle and difficult to read (that’s why you need an MD!), a radiologist would perform the same steps you just did. They make observations about the image, relate their findings to their medical knowledge, and make a diagnosis.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/radPost_mammo.png&quot; alt=&quot;Example Mammograms&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As it turns out, &lt;a href=&quot;http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html&quot;&gt;deep learning algorithms&lt;/a&gt; excel at the same kind of analysis and pattern recognition that a radiologist does. The algorithms will examine thousands of images of every type and eventually learn how to distinguish them based on different sized sections of the image. There is potential to see patterns that would elude even best radiologists, detect exquisitely specific changes in a single person’s history, and totally transform radiology. This application could help to treat breast cancer more effectively, at lower cost, with fewer resources.
Of course, there have already been efforts at individual academic medical centers in mammographic screening. Big breakthroughs in breast imaging are just starting to come out in the high-impact journals.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Recently, &lt;a href=&quot;http://www.nature.com/articles/srep27327&quot;&gt;Wang et al.&lt;/a&gt; were able to detect &lt;a href=&quot;http://www.mayoclinic.org/symptoms/breast-calcifications/basics/definition/sym-20050834&quot;&gt;microcalcifications&lt;/a&gt; in mammograms, an effective early indicator of breast cancer. They used a data set of approximately 1200 images to train a Stacked Auto-Encoder deep learning architecture, and were able to identify microcalcifications with an &lt;a href=&quot;http://healthcare.ai/blog/2016/12/15/model-evaluation-using-roc-curves/&quot;&gt;AUC&lt;/a&gt; of 0.87.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In another study, &lt;a href=&quot;http://www.nature.com/articles/srep24454&quot;&gt;Cheng et al.&lt;/a&gt; used a Stacked Denoising Auto-Encoder to differentiate malignant breast lesions from benign in 550 &lt;a href=&quot;http://www.cancer.org/cancer/breast-cancer/screening-tests-and-early-detection/breast-ultrasound.html&quot;&gt;ultrasound&lt;/a&gt; images. They achieved an AUC of 0.90.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Finally, &lt;a href=&quot;https://arxiv.org/abs/1612.00542&quot;&gt;Levy et al.&lt;/a&gt; trained several existing Convolutional Neural Network (CNN) architectures to classify pre-labeled regions from 1800 mammograms. Their GoogLeNet CNN was able to distinguish malignant from benign with precision of 0.92 and recall of 0.93.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If we were to move forward with a deep learning model for breast imaging, reproducing one of these studies would be a good starting point. Screening mammograms are probably the best place to start, due to their high volume in healthcare. In addition to breast imaging, &lt;a href=&quot;http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7463094&quot;&gt;this excellent review&lt;/a&gt; points out several other promising areas to begin. Additional promising areas are in lung cancer screening from CT images and automated tissue labeling of brain MRI. In lung cancer screening and automatic brain segmentation:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/ftp/arxiv/papers/1611/1611.06651.pdf&quot;&gt;Yang et al.&lt;/a&gt; employed a CNN to classify known lung nodules as either malignant or benign from 4 separate data sets. Some of their models failed to generalize well, but others could achieve extremely high classification accuracy on the validation set.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the same paper as their breast study, &lt;a href=&quot;http://www.nature.com/articles/srep24454&quot;&gt;Cheng et al.&lt;/a&gt; classified 1400 lung nodules from chest CT images. They saw AUC as high as 0.98 depending on the algorithm configuration.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S1361841516300330&quot;&gt;Havaei et al.&lt;/a&gt; present their state of the art segmentation of glioblastoma brain tumors from MRI images. They use a CNN to segment a competition data set with practical speed (0.5 to 3 minutes) and high accuracy.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It would be crazy to try to cover every bit of research in this post. There are countless studies to cover beyond those discussed here, plus annual conferences such as &lt;a href=&quot;https://nips.cc/&quot;&gt;NIPS&lt;/a&gt; and &lt;a href=&quot;https://2017.icml.cc/&quot;&gt;ICML&lt;/a&gt;. However, it would be worth mentioning a few areas of research from the private sector.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Google DeepMind &lt;a href=&quot;https://deepmind.com/applied/deepmind-health/&quot;&gt;recently announced a health initiative&lt;/a&gt; and has been working to identify diabetic retinopathy from images of the eye.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The San Francisco-based start-up, &lt;a href=&quot;http://www.enlitic.com/&quot;&gt;Enlitic&lt;/a&gt;, works to develop deep learning methods for the use cases above and has partnered with several hospitals already.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://siris-medical.com/&quot;&gt;Siris Medical&lt;/a&gt; works to use past patient data to automate radiation treatment planning and brain segmentation.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, many of the data sets that were used in the studies mentioned above are open source and freely available.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.visceral.eu/&quot;&gt;Visceral&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://image-net.org/&quot;&gt;ImageNet&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.kaggle.com/c/data-science-bowl-2017&quot;&gt;Kaggle Data Science Bowl&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://grand-challenge.org/All_Challenges/&quot;&gt;Grand Challenge&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://imaging.cancer.gov/programsandresources/informationsystems/lidc&quot;&gt;Lung Image Database Consortium&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://braintumorsegmentation.org/&quot;&gt;Multimodal Brain Tumor Segmentation Challenge&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Breast screening, lung screening, and brain segmentation are the some of the most popular and promising applications of deep learning in healthcare today. Early research in these areas has shown encouraging results, and there is a lot of effort toward advancements. Furthermore, these areas have the potential to benefit a lot of patients as they include common diseases, treatments, and procedures. It’s an exciting time to be in healthcare machine learning, and we look forward to implementing deep learning into our package.   Thanks for reading, and please feel free to &lt;a href=&quot;http://healthcare.ai/contact&quot;&gt;reach out&lt;/a&gt; with questions!&lt;/p&gt;</content><author><name>Mike Mastanduno</name></author><category term="literature" /><summary type="html">This blog will surveys recent research in deep learning and radiology.</summary></entry><entry><title type="html">Using R for healthcare data analysis</title><link href="/blog/2017/01/17/using-r-for-data-analysis/" rel="alternate" type="text/html" title="Using R for healthcare data analysis" /><published>2017-01-17T11:12:11-07:00</published><updated>2017-01-17T11:12:11-07:00</updated><id>/blog/2017/01/17/using-r-for-data-analysis</id><content type="html" xml:base="/blog/2017/01/17/using-r-for-data-analysis/">&lt;p&gt;When working with data in healthcare, business intelligence (BI) folks often turn to tools like Excel, SSMS, Tableau, and Qlik. Typically, multiple tools will be used when analyzing a dataset. Sometimes the analyst will use Excel to look at the data, get a sense for how the columns are distributed, perhaps make a histogram or scatterplot. Often, analysts will later turn to Qlik and/or Tableau to provide an interactive app, often hosted on a dedicated server so folks in other departments can explore the same data. In this same process, the analyst or data architect may start by querying the database in SSMS to do some simple counts and group bys in an effort to understand the data at a high-level.&lt;/p&gt;

&lt;p&gt;Is that split workflow the most efficient way of doing things? Is there a tool that might provide a streamlined analysis, both providing a way to understand the high-level, as well as offering interactive apps for entire departments? While the tools mentioned above are certainly fantastic, we feel that &lt;strong&gt;R&lt;/strong&gt; could help make life a lot easier for BI professionals.&lt;/p&gt;

&lt;p&gt;While we don’t want to oversell its abilities, think of how often analysts turn to the above tools to do things like:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Understanding how data is distributed&lt;/li&gt;
  &lt;li&gt;Finding how particular columns are correlated&lt;/li&gt;
  &lt;li&gt;Offering pivot tables&lt;/li&gt;
  &lt;li&gt;Making histograms or scatterplots&lt;/li&gt;
  &lt;li&gt;Grouping by a column of interest and plotting a trend&lt;/li&gt;
  &lt;li&gt;Calculating statistics (like standard deviations, t-tests, quantiles)&lt;/li&gt;
  &lt;li&gt;Creating interactive visualizations for others&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You may be surprised to hear that R can also do those things, and do them &lt;em&gt;well&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;If you’re using Excel for things like financial modeling, and/or have the need to input data frequently, then moving to R won’t make sense. We’ll be the first to say that Excel can be a super effective tool.&lt;/p&gt;

&lt;p&gt;But, if you’re often doing analysis using the tools mentioned above, we’re excited to help you see what R can do. Besides the above, here are other benefits of R compared to Excel/Qlik:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Lends itself to source control&lt;/li&gt;
  &lt;li&gt;Makes your work easily reproducible&lt;/li&gt;
  &lt;li&gt;Enables you to tell a data story (combining process and presentation in a notebook)&lt;/li&gt;
  &lt;li&gt;Allows one to more easily find and fix errors&lt;/li&gt;
  &lt;li&gt;Makes it easy to work on very large datasets&lt;/li&gt;
  &lt;li&gt;Offers machine learning&lt;/li&gt;
  &lt;li&gt;It’s free&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We’ll try to illustrate the analytical capabilities of R in a series of blog posts. Each blog post will contain an R notebook that’ll have explanations, R code, and R plots to help you get stated. &lt;a href=&quot;http://healthcare.ai/notebooks/IntroHealthDataAnalysisInR.nb.html&quot;&gt;Here’s the first notebook in this series&lt;/a&gt;. Enjoy!&lt;/p&gt;</content><author><name>Levi Thatcher</name></author><category term="analysis" /><summary type="html">When working with data in healthcare, business intelligence (BI) folks often turn to tools like Excel, SSMS, Tableau, and Qlik. Typically, multiple tools will be used when analyzing a dataset. Sometimes the analyst will use Excel to look at the data, get a sense for how the columns are distributed, perhaps make a histogram or scatterplot. Often, analysts will later turn to Qlik and/or Tableau to provide an interactive app, often hosted on a dedicated server so folks in other departments can explore the same data. In this same process, the analyst or data architect may start by querying the database in SSMS to do some simple counts and group bys in an effort to understand the data at a high-level.</summary></entry><entry><title type="html">Contributing to open source software development using Github</title><link href="/blog/2017/01/12/open-source-and-git/" rel="alternate" type="text/html" title="Contributing to open source software development using Github" /><published>2017-01-12T01:32:11-07:00</published><updated>2017-01-12T01:32:11-07:00</updated><id>/blog/2017/01/12/open-source-and-git</id><content type="html" xml:base="/blog/2017/01/12/open-source-and-git/">&lt;p&gt;The purpose of this post is to help you become familiar with &lt;em&gt;Git&lt;/em&gt;, an essential part of contributing to &lt;a href=&quot;http://healthcare.ai&quot;&gt;healthcare.ai&lt;/a&gt;. Git is essentially a collaboration tool for software developers, and &lt;a href=&quot;http://github.com&quot;&gt;&lt;em&gt;Github&lt;/em&gt;&lt;/a&gt; is the accompanying online storage platform. If you have been reading about healthcare.ai, you probably know that it is an &lt;a href=&quot;http://healthcare.ai/#why&quot;&gt;&lt;em&gt;open source&lt;/em&gt; software package&lt;/a&gt;. Open source means that we aren’t hiding anything from our users. They can use the package, view the contents, and modify the package for their particular needs. We chose to make healthcare.ai open source for two major reasons:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Anyone can contribute to its development.&lt;/strong&gt; When an open source package becomes popular, there could be hundreds of people working on making it better!&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;The rising tide will raise all boats.&lt;/strong&gt; We are trying to be a leader in healthcare machine learning, and hoping that our efforts will be visible, benefit the community, and benefit patient care.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As our team (and number of contributors) grows, it doesn’t make sense to share our code using email attachments. We make thousands of changes on hundreds of files. Instead, we use Git. Git is an industry standard service that makes it easy for large teams to collaborate on code, keep files safe from unwanted changes through &lt;em&gt;version control&lt;/em&gt;, and facilitate code review before changes are published.&lt;/p&gt;

&lt;p&gt;The idea of Git is pretty easy to follow, but the vocabulary can be a little confusing at first. There are &lt;a href=&quot;https://guides.github.com/introduction/flow/&quot;&gt;many&lt;/a&gt; &lt;a href=&quot;http://git.huit.harvard.edu/guide/&quot;&gt;great&lt;/a&gt; &lt;a href=&quot;https://guides.github.com/activities/hello-world/&quot;&gt;resources&lt;/a&gt; about Git online, but sometimes they are too basic for specific issues, or too complicated and lacking explanation. Hopefully you find the ideas below to be both useful and informative.&lt;/p&gt;

&lt;p&gt;Imagine that you are writing a paper. You write some, save the file: “myPaper_version1.doc.” You write more, save version 2, version 3… Essentially, this is what Git is doing. The codebase, or repository, is written line by line, and each change can be stored as a &lt;em&gt;commit&lt;/em&gt;. The commits flow linearly, and if you don’t like the latest changes, you can roll back to the previous commit. This &lt;a href=&quot;http://rogerdudler.github.io/git-guide/files/git_cheat_sheet.pdf&quot;&gt;excellent cheat sheet shows that process&lt;/a&gt;, where each circle represents a commit.&lt;/p&gt;

&lt;p&gt;I’ll focus on explaining five topics that are likely to help you collaborate.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Local vs source&lt;br /&gt;
A code repository lives at &lt;a href=&quot;http://github.com&quot;&gt;www.github.com&lt;/a&gt;, and you’d like to work on it. The &lt;em&gt;master&lt;/em&gt; source is online. You need to &lt;em&gt;clone&lt;/em&gt; or &lt;em&gt;fork&lt;/em&gt; the repo to create a local copy on your computer. You can make improvements, commit them, and document each change. Verify that your code is working, then, they can be &lt;em&gt;merged&lt;/em&gt; into the source copy.&lt;/li&gt;
  &lt;li&gt;Pushing and Pulling&lt;br /&gt;
When you are ready to submit your changes to the master, you must first &lt;em&gt;pull&lt;/em&gt; changes from the master source to your local copy. This ensures that you have the latest changes from the master in your local copy. Then you can &lt;em&gt;push&lt;/em&gt; your changes up to the online master. If you take a few weeks away from the code, make sure you pull the latest changes into your local copy before starting your work.
    &lt;ul&gt;
      &lt;li&gt;Pull changes from the master source to your local copy: &lt;code class=&quot;highlighter-rouge&quot;&gt;git pull&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Push your local changes to the online master: &lt;code class=&quot;highlighter-rouge&quot;&gt;git push&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Branches&lt;br /&gt;
&lt;a href=&quot;http://github.com/HealthCatalystSLC/healthcareai-r/branches/&quot;&gt;&lt;em&gt;Branches&lt;/em&gt;&lt;/a&gt; are used to help keep large changes, feature additions, etc. separate from the master source. They allow you to “break the code to fix it” without worrying that you are going to ruin the master. If you create a topic branch to work in, you now have two separate local copies: your local master, and your local branch. You can make ongoing changes to the code in your branch, then switch back to the master to actually use code that you know is working.
    &lt;ul&gt;
      &lt;li&gt;Create a new topic branch or switch to an existing topic branch: &lt;code class=&quot;highlighter-rouge&quot;&gt;git checkout -b nameofbranch&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Merging&lt;br /&gt;
When the topic branch you’ve been developing is done, documented, and functional, it’s ready to be merged back into the master branch. Again, update your local master, as other people could have been working on the code while you were. Merge the local master into your local branch (the ordering can be disputed, but this is how we do it at healthcare.ai), allowing you to test for functionality with all the latest changes. Finally, push the local branch up to the server and ask for review.
    &lt;ul&gt;
      &lt;li&gt;Check out the latest online master and merge into the local topic branch: &lt;code class=&quot;highlighter-rouge&quot;&gt;git merge origin/master&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Make sure it all still works!&lt;/li&gt;
      &lt;li&gt;Push the local branch to the server for review: &lt;code class=&quot;highlighter-rouge&quot;&gt;git push&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Pull Requests&lt;br /&gt;
After pushing a completed topic branch up to the server, the &lt;em&gt;pull request&lt;/em&gt; acts as a request for code review. The term pull request &lt;del&gt;makes no sense to me&lt;/del&gt; &lt;a href=&quot;https://www.quora.com/GitHub-Why-is-the-pull-request-called-pull-request&quot;&gt;Got it!&lt;/a&gt; Another developer will look through your changes and documentation, ask for revisions, and eventually approve the pull request. After this happens, the branch will be merged into the online master. Anyone who clones the master will now get your changes, and the branch can be deleted.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There is a lot of vocabulary in this post, but we hope that it’s helpful to see these things defined in context of how they are used. Seasoned developers forget that Git language can be hard to interpret since it’s such an important part of their everyday work. The truth is, it’s a barrier to entry for the casual contributor. However, it’s extremely important for the health of a large code project and the team that works on it.&lt;/p&gt;

&lt;p&gt;If you’ve become motivated to get down to business, make an account on &lt;a href=&quot;https://github.com/&quot;&gt;Github&lt;/a&gt;, &lt;a href=&quot;https://github.com/HealthCatalystSLC/healthcareai-r/blob/master/CONTRIBUTING.md#clone-healthcareai-r-repo&quot;&gt;clone our repo&lt;/a&gt;, and help us shape the future of healthcare machine learning! There are more detailed instructions in the &lt;a href=&quot;https://github.com/HealthCatalystSLC/healthcareai-r/blob/master/README.md&quot;&gt;readme&lt;/a&gt; and &lt;a href=&quot;https://github.com/HealthCatalystSLC/healthcareai-r/blob/master/CONTRIBUTING.md&quot;&gt;contributing&lt;/a&gt; files, and you can feel free to &lt;a href=&quot;http://healthcare.ai/contact.html&quot;&gt;send questions&lt;/a&gt; our way.&lt;/p&gt;</content><author><name>Mike Mastanduno</name></author><category term="workflow" /><summary type="html">This blog will describe the motivation and workflow of Git version control</summary></entry><entry><title type="html">Know your business question: A focus on readmissions</title><link href="/blog/2017/01/11/know-your-business-question-a-focus-on-readmissions/" rel="alternate" type="text/html" title="Know your business question: A focus on readmissions" /><published>2017-01-11T21:00:00-07:00</published><updated>2017-01-11T21:00:00-07:00</updated><id>/blog/2017/01/11/know-your-business-question-a-focus-on-readmissions</id><content type="html" xml:base="/blog/2017/01/11/know-your-business-question-a-focus-on-readmissions/">&lt;p&gt;As time goes on, we will not only discuss healthcare machine learning (ML) and health in the US at a high level, but also specific ways ML might help drive outcomes improvements. Many health systems are working on reducing their readmission rate—which is often considered a measure of quality of care and can be tied to penalties. For  hospital systems progressing toward ML for readmissions—or any measure—the first step is to identify your most important business questions; the next step is creating a suitable dataset to create the model. There are often several points where business logic dictates decisions related to the dataset and whether one or multiple models are needed to help with a specific process. That’s certainly the case when creating readmission risk models.&lt;/p&gt;

&lt;p&gt;Readmission risk models improve patient quality of life and decrease mortality by providing extra care or surveillance to high-risk patients. Implementing a readmission risk model could serve two different purposes:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Identifying high-risk &lt;em&gt;observational&lt;/em&gt; and &lt;em&gt;inpatient&lt;/em&gt; patients as soon after their admission as possible to answer this question: &lt;em&gt;Which in-hospital patients are most at risk for readmission?&lt;/em&gt; By answering this question, doctors, nurses, and in-hospital staff can intervene to try to lower a patient’s readmission risk.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Identifying high-risk &lt;em&gt;discharged&lt;/em&gt; patients as soon after their discharge as possible to answer this question: &lt;em&gt;Which discharged patients are most at risk of readmission?&lt;/em&gt; By answering this question, hospital support staff and transitional services can intervene to try to lower a patient’s readmission risk.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now, you might think, “Those are the same use case: they both predict readmissions, even sometimes for the same patients.” But remember, an ML model is only valuable if it provides actionable insight. Nurses are in the optimal position to help when the patient is in the hospital. Furthermore, as we &lt;a href=&quot;http://healthcare.ai/blog/2017/01/06/data-leakage-in-healthcare-machine-learning/&quot;&gt;discussed at length&lt;/a&gt; last week, the data sources for these two use cases are very different. The type and amount of information available at discharge is different from what is available at admission. The best ML models are built custom to a specific dataset and use case. One risk model is not sufficient to answer both questions, and accuracy would almost certainly be compromised if they were naively combined. Though similar, these questions need models that relate to different types of patients and are targeted toward different interventions. Under the hood of the models, we need to use different datasets (to avoid &lt;a href=&quot;http://healthcare.ai/blog/2017/01/06/data-leakage-in-healthcare-machine-learning/&quot;&gt;data leakage&lt;/a&gt;)—and maybe even different algorithms. The best ML model will always be built to answer a specific question, tailored to specific data, and targeted toward the most effective intervention.&lt;/p&gt;

&lt;p&gt;OK, so we know we need two models for readmission risk modeling. But to complicate things even more, we must keep in mind that the layman definition of readmission and the &lt;a href=&quot;https://www.cms.gov/Medicare/Medicare-Fee-for-Service-Payment/PhysicianFeedbackProgram/Downloads/2014-ACR-MIF.pdf&quot;&gt;CMS definition of readmission&lt;/a&gt; are also subtly different. We must base our outcome variable on a definition very similar to that of CMS because it is consistent with the way many hospital systems track and report outcomes, and the CMS definition of readmissions is ultimately the measure hospital systems are trying to affect. Knowing that definition is critical to building a model to address it.&lt;/p&gt;

&lt;p&gt;Per the CMS definition, patients in the hospital can be &lt;em&gt;emergency department&lt;/em&gt;, &lt;em&gt;observational&lt;/em&gt;, or &lt;em&gt;inpatient&lt;/em&gt;. To be considered an unplanned readmission patients must initially be discharged from the inpatient setting. A patient that is discharged from the emergency department or observation setting cannot be readmitted (0% probability) because they did not meet the inpatient requirement pertaining to the inpatient index admission. Similarly, even after a patient is admitted to the inpatient setting, we still do not yet know their what their discharge disposition or discharge diagnosis will be. If the patient leaves against medical advice or is assigned a cancer-related discharge, for instance, they meet a different set of exclusion criteria and cannot be readmitted (again, 0% probability). While the specific criteria behind the definition of a readmission makes practical sense, it creates a couple of challenges to training, testing, and deploying a readmission risk model that is to be leveraged while patients are still in the hospital.&lt;/p&gt;

&lt;p&gt;At the end of the day, we need to develop the model using the same data that we want it to perform well on in production. For the &lt;em&gt;in-hospital&lt;/em&gt; use case, &lt;em&gt;observational&lt;/em&gt; and &lt;em&gt;emergency department&lt;/em&gt; patients that would ultimately be excluded at discharge must also be excluded from model development. These patients may skew the model toward predicting 0% readmission probability since they are guaranteed to be 0% risk as defined by CMS. This puts us in a tight spot. When &lt;a href=&quot;http://healthcare.ai/blog/2016/12/15/model-evaluation-using-roc-curves/&quot;&gt;evaluating the model&lt;/a&gt;, it may appear to have higher accuracy by skewing toward low probabilities because it was improperly trained on data that should have been excluded. For the post-discharge use case, the discharge type is available and the model can match the CMS definition more closely. This will likely lead to increased accuracy overall, as more use-case specific data is available.&lt;/p&gt;

&lt;p&gt;From our experience, understanding the definition of the readmission outcome variable, the specific use case, and the timing/target is crucial. There is clearly a trade-off between timeliness and accuracy, and to have the greatest impact on patient outcomes, it is important to develop readmission risk models based on data that reflects the use case in production. Again, the best ML model should answer a specific question, using specific data, with an actionable result. Keep these things in mind and your models will improve.&lt;/p&gt;

&lt;p&gt;Thanks for reading and please &lt;a href=&quot;http://healthcare.ai/contact&quot;&gt;reach out&lt;/a&gt; with any questions or comments!&lt;/p&gt;</content><author><name>Taylor Larsen</name></author><category term="overview" /><summary type="html">This post describes the importance of understanding the business questions, use cases, and data when creating a readmission risk model</summary></entry><entry><title type="html">Which regions of the US are healthy?</title><link href="/blog/2017/01/08/us-health-by-county/" rel="alternate" type="text/html" title="Which regions of the US are healthy?" /><published>2017-01-08T16:00:00-07:00</published><updated>2017-01-08T16:00:00-07:00</updated><id>/blog/2017/01/08/us-health-by-county</id><content type="html" xml:base="/blog/2017/01/08/us-health-by-county/">&lt;p&gt;While our previous posts have focused on healthcare machine learning, we’re also excited to post analyses of health data using R and Python. We do this to hopefully elevate the national discussion around health data, enhance the community’s understanding of health in the United States (US), and provide guidance as to how communities and health systems might increase the quality and length of people’s lives. Health Catalyst is an outcomes improvement company, and we realize that the inpatient setting is only one of several venues that affect a person’s health trajectory. Understanding the big picture of health is another way to approach outcomes improvements. These posts will not only attempt to educate on findings about health, but also on how to use R/Python for health data analysis, so we’ll always post links to the &lt;a href=&quot;https://gist.github.com/levithatcher/070496ca48c165d7ced37e0ffcd24dc7&quot;&gt;relevant code&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Those who have been in healthcare for any significant amount of time have heard that social determinants of health (SDOH) are important to healthcare outcomes. While it’s hard to overstate the importance of these factors, they’re often not well understood and overshadowed by inpatient optimizations when discussing outcomes improvement. This post is the first in a series where we’ll attempt to untangle the drivers behind population health differences across the US. Today we’ll talk about where the US stands from region to region in terms of social determinants of health. In subsequent posts we’ll discuss whether these differences mostly related to income, air pollution, access to healthy food, long commutes, access to healthcare, opioid addiction, or alcohol abuse.&lt;/p&gt;

&lt;p&gt;To try and answer that we’ll &lt;a href=&quot;https://gist.github.com/levithatcher/070496ca48c165d7ced37e0ffcd24dc7&quot;&gt;use R&lt;/a&gt; and &lt;a href=&quot;http://www.countyhealthrankings.org/sites/default/files/2015%20CHR%20Analytic%20Data.csv&quot;&gt;data&lt;/a&gt; from &lt;a href=&quot;http://www.countyhealthrankings.org/&quot;&gt;countyhealthrankings.org&lt;/a&gt;, which is a fantastic resource on SDOH comparisons by county. We’ll start by presenting a choropleth map of median household 2015 &lt;a href=&quot;http://www.countyhealthrankings.org/measure/median-household-income&quot;&gt;income by county&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/Post10CountyHealthOverview/MedianIncomeByCounty.jpg&quot; alt=&quot;IncomeByCounty&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Of course, what we find is that there are large regional differences in household income. Broadly, the Northeast, the West Coast, and metropolitan areas are associated with higher personal incomes, compared with rural areas and the South. Note that occasionally there is high intra-state variation, such as in Texas, compared to the more uniform median incomes across counties in Minnesota. But how do these regional variations in incomes correspond with healthcare outcomes? Data on &lt;a href=&quot;http://www.countyhealthrankings.org/measure/low-birthweight&quot;&gt;Low Birth-Weight&lt;/a&gt; (LBW), i.e., live births under ~5 lbs 8 oz (2500 g), provides a &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/7633862&quot;&gt;helpful link&lt;/a&gt; between social determinants and actual healthcare outcomes, as&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“&lt;a href=&quot;http://www.countyhealthrankings.org/measure/low-birthweight&quot;&gt;LBW indicates maternal exposure to health risks&lt;/a&gt; in all categories of health factors, including her health behaviors, access to health care the social and economic environment she inhabits, and environmental risks to which she is exposed. In terms of the infant’s health outcomes, LBW serves as a predictor of premature mortality and/or morbidity over the life course and for potential cognitive development problems.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Pulling data from &lt;a href=&quot;http://www.countyhealthrankings.org/measure/low-birthweight&quot;&gt;here&lt;/a&gt;, and &lt;a href=&quot;https://gist.github.com/levithatcher/070496ca48c165d7ced37e0ffcd24dc7&quot;&gt;processing with R&lt;/a&gt;, we plot the percentage of county live births with birth-weight under 5 lbs 8 oz:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/Post10CountyHealthOverview/LowBirthWeightByCounty.jpg&quot; alt=&quot;LBWByCounty&quot; /&gt;&lt;/p&gt;

&lt;p&gt;While there’s a lot that could be unpacked here, we’ll simply note that the same regions that had lower personal incomes also have a higher percentage of LBW. Not a huge surprise—it is surprising, however, how much intra-state variation is present (like in NV and CO) and how the Deep South has rates of LBW that are often twice that of Minnesota and Wisconsin.&lt;/p&gt;

&lt;p&gt;While income appears to be associated with new-born morbidity and mortality, how does it affect populations later in life? We use &lt;a href=&quot;http://www.countyhealthrankings.org/measure/premature-death-ypll&quot;&gt;premature mortality&lt;/a&gt; &lt;a href=&quot;http://www.countyhealthrankings.org/sites/default/files/2015%20CHR%20Analytic%20Data.csv&quot;&gt;data&lt;/a&gt; from &lt;a href=&quot;http://www.countyhealthrankings.org/&quot;&gt;countyhealthrankings.org&lt;/a&gt;, where a premature death is defined as occurring before 70 years of age, to answer this question. For each county, per 100k people, the years of death before 75 are summed. Think of it as incidence of early death, per county:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/Post10CountyHealthOverview/PrematureDeathByCounty.jpg&quot; alt=&quot;PrematureDeathByCounty&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Compared to LBW, it appears that premature mortality more closely corresponds with median county income. Note how the high incidence of premature mortality across Arkansas, Tennessee, and Kentucky closely tracks income (comparing with the first figure). Broadly, Appalachia appears to suffer more from deaths of prime-age adults compared to LBW (while the South appears to suffer greatly from both). Note that while the rust belt (i.e., PA, OH, IN, MI, IL, and WI) certainly has other issues, they seem to be doing a good job of keeping prime-age people alive, especially compared to Appalachia and the Deep South.&lt;/p&gt;

&lt;p&gt;While it’s old-hat to say that population health in the Deep South isn’t fantastic, let’s go one step further and see which counties in the US punch above their weight when it comes to using resources effectively. In other words, which counties are doing well for how poor they are. This is the metric that will let us understand what it is that poor and middle-income counties do well in terms of keeping people healthy.&lt;/p&gt;

&lt;p&gt;To get at this, we create percentiles for each county in terms of LBW (where 100 is best) and then subtract (for each county) the percentiles for income. We can call this the county Punch-Above-Their-Weight Index or PATWI for short. Before plotting the entire country, let’s look at LBW PATWI for the top ten counties in terms of income:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/Post10CountyHealthOverview/TableHighestIncomeAndLBW.png&quot; alt=&quot;TableHighestIncomeandLBW&quot; /&gt;&lt;/p&gt;

&lt;p&gt;First, in this and the following tables, the PATWI column is just the fifth column minus the fourth column. While the counties above do have good scores in terms of LBW, it’s difficult for rich counties to punch above their weight, since they’re the 800-lb gorillas. We do, however, see the benefit of the PATWI, since the richest counties in the US &lt;em&gt;aren’t&lt;/em&gt; those with the best LBW scores.&lt;/p&gt;

&lt;p&gt;Which counties have the best LBW, considering their income?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/Post10CountyHealthOverview/TableHighestLBWPATWIAndIncome.png&quot; alt=&quot;TableHighestPATWIAndIncome&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It’s impressive that these counties, which have median incomes one-third of that of Loudoun County (VA), have better LBW scores than Loudoun County. Note that the best LBW PATWI scores come from a scattered grouping of states, although, Michigan (MI) impressively has three entries and Missouri (MO) has two. That’s definitely good news for MI and MO public health officials. Note that this PATWI measure doesn’t just bias towards the poorest counties, either. While Buffalo County, SD—perhaps the poorest county in the nation—has a high LDW PATWI score, none of the counties from the Deep South or Appalachia show up in this list. Here’s LDW PATWI score for all counties:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/Post10CountyHealthOverview/LBWComparedToIncomeByCounty.jpg&quot; alt=&quot;LBWComparedToIncome&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that positive values here denote that the county is doing better at LBW than expected, based on their income*. Amongst other findings, we see that even though northern half of West Virginia is quite poor, they’re doing better than expected at helping mothers carry and deliver healthy babies. The Deep South, however, is doing about as poorly (or worse) than one would expect by looking at their income along. Note that Missouri is doing better than expected, as is rural Oregon, Minneapolis, and Wisconsin. Recall that metro areas tend to be richer than rural areas, so it’s impossible for richest areas like the Bay Area and NYC tend to have a high score (since their percentiles can’t go over 100)—this limitation makes this plot more of a measure of how middle and low income counties are doing relative to their income.
Let’s do the same for the premature mortality metric we discussed above—in other words, which counties are doing a great job of avoiding early mortality. We’ll start with the richest counties:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/Post10CountyHealthOverview/TableHighestIncomeAndPrematureDeath.png&quot; alt=&quot;TableHighestIncomeAndLBW&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Yup, the richest counties are great at keeping people alive—even more so than avoiding low birth-weight (compare with the first table above). But, which poor or middle income counties are best at punching above their weight when it comes to keeping their citizens alive?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/Post10CountyHealthOverview/TableHighestPrematureDeathPATWIAndIncome.png&quot; alt=&quot;TableHighestLBWPATWIAndIncome&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It’s impressive how these relatively poor counties are achieving the top 10-20th percentile in terms of avoiding premature deaths (note that we’re using higher percentiles to mean good outcomes). It is an interesting mix of counties, indeed. Santa Cruz (AZ), Presidio County (TX), and Maverick County (TX) border Mexico; Crowley County (CO) has the largest per-capita prison population in the country. Madison County, home to BYU-Idaho, is &lt;a href=&quot;http://www.slate.com/articles/life/map_of_the_week/2012/02/mormon_population_in_the_u_s_an_interactive_map.html&quot;&gt;~80% Mormon&lt;/a&gt;, which likely means that county overall has low rates of alcohol, tobacco, and drug dependency. In a future post we’ll go into how certain counties are punching above their weight, and for now offer the national view of premature-death PATWI score:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/Post10CountyHealthOverview/PrematureDeathComparedToIncomeByCounty.jpg&quot; alt=&quot;PrematureDeathComparedToIncome&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Recall that darker means that the county is doing well. At this level, what we notice is that Wisconsin, Michigan, and Missouri are doing a good job keeping prime-age people alive compared to Nevada and Wyoming. Knowing why this is occurring can significantly change how a health system interacts with its patients.&lt;/p&gt;

&lt;p&gt;At Health Catalyst, we strive to understand the significant way social determinants of health can affect decision making at health systems from the Deep South to the Bay Area. Our clients are located all across the continent. High-value improvements in one health system are not necessarily the best everywhere, and we’d be cheating ourselves to only look for improvements at inpatient units. We’re committed to leveraging the tools of population health to improve patient outcomes across the board. 
This post is the first of many on population health, and in a future post we’ll dig more into which social determinants are most driving regional variations of LBW and premature death.&lt;/p&gt;

&lt;p&gt;Thanks for reading and please &lt;a href=&quot;http://healthcare.ai/contact&quot;&gt;reach out&lt;/a&gt; with any questions or comments!&lt;/p&gt;

&lt;p&gt;* The distribution of county median income has a long tail, which means that when subtracting county income percentiles from the corresponding LBW or premature death percentiles, the result is typically positive.&lt;/p&gt;</content><author><name>Levi Thatcher</name></author><category term="overview" /><summary type="html">While our previous posts have focused on healthcare machine learning, we’re also excited to post analyses of health data using R and Python. We do this to hopefully elevate the national discussion around health data, enhance the community’s understanding of health in the United States (US), and provide guidance as to how communities and health systems might increase the quality and length of people’s lives. Health Catalyst is an outcomes improvement company, and we realize that the inpatient setting is only one of several venues that affect a person’s health trajectory. Understanding the big picture of health is another way to approach outcomes improvements. These posts will not only attempt to educate on findings about health, but also on how to use R/Python for health data analysis, so we’ll always post links to the relevant code.</summary></entry></feed>
